{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a03e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo original: raw\\guia_embarazo_parto_2023.pdf\n",
      "Archivo procesado: processed\\guia_embarazo_parto_2023_sin_intro.pdf\n",
      "Archivo encontrado. Tama√±o: 3.54 MB\n",
      "Error: El archivo processed\\guia_embarazo_parto_2023_sin_intro.pdf no existe\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de datos - Gu√≠a de Embarazo y Parto 2023\n",
    "# Paso 1: Eliminar las primeras 120 p√°ginas del PDF\n",
    "\n",
    "import PyPDF2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuraci√≥n de rutas\n",
    "raw_data_path = Path(\"raw/guia_embarazo_parto_2023.pdf\")\n",
    "processed_data_path = Path(\"processed/guia_embarazo_parto_2023_sin_intro.pdf\")\n",
    "\n",
    "# Crear directorio processed si no existe\n",
    "processed_data_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Archivo original: {raw_data_path}\")\n",
    "print(f\"Archivo procesado: {processed_data_path}\")\n",
    "\n",
    "# Verificar que el archivo original existe\n",
    "if not raw_data_path.exists():\n",
    "    print(f\"Error: El archivo {raw_data_path} no existe\")\n",
    "else:\n",
    "    print(f\"Archivo encontrado. Tama√±o: {raw_data_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "# Verificar que el archivo original existe\n",
    "if not processed_data_path.exists():\n",
    "    print(f\"Error: El archivo {processed_data_path} no existe\")\n",
    "else:\n",
    "    print(f\"Archivo encontrado. Tama√±o: {processed_data_path.stat().st_size / (1024*1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db475bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para eliminar las primeras 120 p√°ginas del PDF\n",
    "def remove_first_pages(input_path, output_path, pages_to_remove=120):\n",
    "    \"\"\"\n",
    "    Elimina las primeras 'pages_to_remove' p√°ginas de un PDF\n",
    "    \n",
    "    Args:\n",
    "        input_path (Path): Ruta del archivo PDF original\n",
    "        output_path (Path): Ruta donde guardar el PDF procesado\n",
    "        pages_to_remove (int): N√∫mero de p√°ginas a eliminar desde el inicio\n",
    "    \n",
    "    Returns:\n",
    "        bool: True si el procesamiento fue exitoso, False en caso contrario\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Abrir el PDF original\n",
    "        with open(input_path, 'rb') as input_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(input_file)\n",
    "            pdf_writer = PyPDF2.PdfWriter()\n",
    "            \n",
    "            total_pages = len(pdf_reader.pages)\n",
    "            print(f\"Total de p√°ginas en el PDF original: {total_pages}\")\n",
    "            \n",
    "            if pages_to_remove >= total_pages:\n",
    "                print(f\"Error: Se quieren eliminar {pages_to_remove} p√°ginas, pero el PDF solo tiene {total_pages} p√°ginas\")\n",
    "                return False\n",
    "            \n",
    "            # Agregar las p√°ginas desde la p√°gina 'pages_to_remove' hasta el final\n",
    "            pages_to_keep = total_pages - pages_to_remove\n",
    "            print(f\"Eliminando las primeras {pages_to_remove} p√°ginas\")\n",
    "            print(f\"P√°ginas que se mantendr√°n: {pages_to_keep}\")\n",
    "            \n",
    "            for page_num in range(pages_to_remove, total_pages):\n",
    "                pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "            \n",
    "            # Guardar el nuevo PDF\n",
    "            with open(output_path, 'wb') as output_file:\n",
    "                pdf_writer.write(output_file)\n",
    "            \n",
    "            print(f\"PDF procesado guardado exitosamente en: {output_path}\")\n",
    "            print(f\"Tama√±o del archivo procesado: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el PDF: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c62d8cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO PROCESAMIENTO DEL PDF ===\n",
      "\n",
      "Total de p√°ginas en el PDF original: 623\n",
      "Eliminando las primeras 123 p√°ginas\n",
      "P√°ginas que se mantendr√°n: 500\n",
      "PDF procesado guardado exitosamente en: processed\\guia_embarazo_parto_2023_sin_intro.pdf\n",
      "Tama√±o del archivo procesado: 2.53 MB\n",
      "\n",
      "=== PROCESAMIENTO COMPLETADO EXITOSAMENTE ===\n",
      "‚úÖ Las primeras 120 p√°ginas han sido eliminadas\n",
      "‚úÖ Archivo procesado disponible en: processed\\guia_embarazo_parto_2023_sin_intro.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el procesamiento para eliminar las primeras 120 p√°ginas\n",
    "print(\"=== INICIANDO PROCESAMIENTO DEL PDF ===\")\n",
    "print()\n",
    "\n",
    "# Ejecutar la funci√≥n\n",
    "success = remove_first_pages(raw_data_path, processed_data_path, pages_to_remove=123)\n",
    "\n",
    "if success:\n",
    "    print()\n",
    "    print(\"=== PROCESAMIENTO COMPLETADO EXITOSAMENTE ===\")\n",
    "    print(f\"‚úÖ Las primeras 120 p√°ginas han sido eliminadas\")\n",
    "    print(f\"‚úÖ Archivo procesado disponible en: {processed_data_path}\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"=== ERROR EN EL PROCESAMIENTO ===\")\n",
    "    print(\"‚ùå No se pudo completar el procesamiento del PDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c4688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICACI√ìN DEL ARCHIVO PROCESADO ===\n",
      "\n",
      "üìÑ P√°ginas en el archivo procesado: 500\n",
      "üíæ Tama√±o del archivo procesado: 2.53 MB\n",
      "üìÅ Ubicaci√≥n: c:\\Users\\MSI\\Desktop\\DreamTeam\\RAG-Benchmark\\Data\\processed\\guia_embarazo_parto_2023_sin_intro.pdf\n",
      "\n",
      "üìä Comparaci√≥n:\n",
      "   ‚Ä¢ P√°ginas originales: 623\n",
      "   ‚Ä¢ P√°ginas eliminadas: 120\n",
      "   ‚Ä¢ P√°ginas esperadas: 503\n",
      "   ‚Ä¢ P√°ginas obtenidas: 500\n",
      "   ‚ùå Error: El n√∫mero de p√°ginas no coincide con lo esperado.\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n adicional del archivo procesado\n",
    "print(\"=== VERIFICACI√ìN DEL ARCHIVO PROCESADO ===\")\n",
    "print()\n",
    "\n",
    "if processed_data_path.exists():\n",
    "    # Verificar el nuevo PDF\n",
    "    try:\n",
    "        with open(processed_data_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            new_total_pages = len(pdf_reader.pages)\n",
    "            file_size_mb = processed_data_path.stat().st_size / (1024*1024)\n",
    "            \n",
    "            print(f\"üìÑ P√°ginas en el archivo procesado: {new_total_pages}\")\n",
    "            print(f\"üíæ Tama√±o del archivo procesado: {file_size_mb:.2f} MB\")\n",
    "            print(f\"üìÅ Ubicaci√≥n: {processed_data_path.absolute()}\")\n",
    "            \n",
    "            # Verificar que realmente se eliminaron 120 p√°ginas\n",
    "            with open(raw_data_path, 'rb') as original_file:\n",
    "                original_pdf = PyPDF2.PdfReader(original_file)\n",
    "                original_pages = len(original_pdf.pages)\n",
    "                expected_pages = original_pages - 123\n",
    "                \n",
    "                print()\n",
    "                print(f\"üìä Comparaci√≥n:\")\n",
    "                print(f\"   ‚Ä¢ P√°ginas originales: {original_pages}\")\n",
    "                print(f\"   ‚Ä¢ P√°ginas eliminadas: 120\")\n",
    "                print(f\"   ‚Ä¢ P√°ginas esperadas: {expected_pages}\")\n",
    "                print(f\"   ‚Ä¢ P√°ginas obtenidas: {new_total_pages}\")\n",
    "                \n",
    "                if new_total_pages == expected_pages:\n",
    "                    print(\"   ‚úÖ ¬°Verificaci√≥n exitosa! El n√∫mero de p√°ginas es correcto.\")\n",
    "                else:\n",
    "                    print(\"   ‚ùå Error: El n√∫mero de p√°ginas no coincide con lo esperado.\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al verificar el archivo procesado: {str(e)}\")\n",
    "else:\n",
    "    print(\"‚ùå El archivo procesado no existe. Algo sali√≥ mal en el procesamiento.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f3d0a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASO 2: ELIMINAR √öLTIMAS 98 P√ÅGINAS ===\n",
      "\n",
      "Archivo de entrada (ya sin primeras p√°ginas): processed\\guia_embarazo_parto_2023_sin_intro.pdf\n",
      "Archivo final (sin primeras y √∫ltimas p√°ginas): processed\\guia_embarazo_parto_2023_final.pdf\n",
      "üìÑ P√°ginas actuales en el archivo: 500\n",
      "üóëÔ∏è P√°ginas a eliminar del final: 98\n",
      "üìù P√°ginas que quedar√°n: 402\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Eliminar las √∫ltimas 98 p√°ginas del PDF ya procesado\n",
    "print(\"=== PASO 2: ELIMINAR √öLTIMAS 98 P√ÅGINAS ===\")\n",
    "print()\n",
    "\n",
    "# Configuraci√≥n de rutas para el segundo procesamiento\n",
    "input_path_step2 = Path(\"processed/guia_embarazo_parto_2023_sin_intro.pdf\")  # El archivo que ya procesamos\n",
    "final_processed_path = Path(\"processed/guia_embarazo_parto_2023_final.pdf\")\n",
    "\n",
    "print(f\"Archivo de entrada (ya sin primeras p√°ginas): {input_path_step2}\")\n",
    "print(f\"Archivo final (sin primeras y √∫ltimas p√°ginas): {final_processed_path}\")\n",
    "\n",
    "# Verificar que el archivo de entrada existe\n",
    "if not input_path_step2.exists():\n",
    "    print(f\"‚ùå Error: El archivo {input_path_step2} no existe\")\n",
    "    print(\"Primero necesitas ejecutar el Paso 1 para eliminar las primeras p√°ginas\")\n",
    "else:\n",
    "    with open(input_path_step2, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        current_pages = len(pdf_reader.pages)\n",
    "        print(f\"üìÑ P√°ginas actuales en el archivo: {current_pages}\")\n",
    "        print(f\"üóëÔ∏è P√°ginas a eliminar del final: 98\")\n",
    "        print(f\"üìù P√°ginas que quedar√°n: {current_pages - 98}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df3c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para eliminar las √∫ltimas p√°ginas del PDF\n",
    "def remove_last_pages(input_path, output_path, pages_to_remove_from_end=98):\n",
    "    \"\"\"\n",
    "    Elimina las √∫ltimas 'pages_to_remove_from_end' p√°ginas de un PDF\n",
    "    \n",
    "    Args:\n",
    "        input_path (Path): Ruta del archivo PDF de entrada\n",
    "        output_path (Path): Ruta donde guardar el PDF procesado\n",
    "        pages_to_remove_from_end (int): N√∫mero de p√°ginas a eliminar desde el final\n",
    "    \n",
    "    Returns:\n",
    "        bool: True si el procesamiento fue exitoso, False en caso contrario\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Abrir el PDF de entrada\n",
    "        with open(input_path, 'rb') as input_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(input_file)\n",
    "            pdf_writer = PyPDF2.PdfWriter()\n",
    "            \n",
    "            total_pages = len(pdf_reader.pages)\n",
    "            print(f\"Total de p√°ginas en el PDF actual: {total_pages}\")\n",
    "            \n",
    "            if pages_to_remove_from_end >= total_pages:\n",
    "                print(f\"Error: Se quieren eliminar {pages_to_remove_from_end} p√°ginas del final, pero el PDF solo tiene {total_pages} p√°ginas\")\n",
    "                return False\n",
    "            \n",
    "            # Calcular cu√°ntas p√°ginas mantener (desde el inicio hasta antes de las √∫ltimas que queremos eliminar)\n",
    "            pages_to_keep = total_pages - pages_to_remove_from_end\n",
    "            print(f\"Eliminando las √∫ltimas {pages_to_remove_from_end} p√°ginas\")\n",
    "            print(f\"P√°ginas que se mantendr√°n: {pages_to_keep} (p√°ginas 1 a {pages_to_keep})\")\n",
    "            \n",
    "            # Agregar solo las p√°ginas que queremos mantener (desde 0 hasta pages_to_keep-1)\n",
    "            for page_num in range(0, pages_to_keep):\n",
    "                pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "            \n",
    "            # Guardar el nuevo PDF\n",
    "            with open(output_path, 'wb') as output_file:\n",
    "                pdf_writer.write(output_file)\n",
    "            \n",
    "            print(f\"PDF procesado guardado exitosamente en: {output_path}\")\n",
    "            print(f\"Tama√±o del archivo final: {output_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el PDF: {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c0e9ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EJECUTANDO ELIMINACI√ìN DE √öLTIMAS P√ÅGINAS ===\n",
      "\n",
      "Total de p√°ginas en el PDF actual: 500\n",
      "Eliminando las √∫ltimas 98 p√°ginas\n",
      "P√°ginas que se mantendr√°n: 402 (p√°ginas 1 a 402)\n",
      "PDF procesado guardado exitosamente en: processed\\guia_embarazo_parto_2023_final.pdf\n",
      "Tama√±o del archivo final: 1.95 MB\n",
      "\n",
      "=== PROCESAMIENTO PASO 2 COMPLETADO EXITOSAMENTE ===\n",
      "‚úÖ Las √∫ltimas 98 p√°ginas han sido eliminadas\n",
      "‚úÖ Archivo final disponible en: processed\\guia_embarazo_parto_2023_final.pdf\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el procesamiento para eliminar las √∫ltimas 98 p√°ginas\n",
    "print(\"=== EJECUTANDO ELIMINACI√ìN DE √öLTIMAS P√ÅGINAS ===\")\n",
    "print()\n",
    "\n",
    "# Ejecutar la funci√≥n\n",
    "success_step2 = remove_last_pages(input_path_step2, final_processed_path, pages_to_remove_from_end=98)\n",
    "\n",
    "if success_step2:\n",
    "    print()\n",
    "    print(\"=== PROCESAMIENTO PASO 2 COMPLETADO EXITOSAMENTE ===\")\n",
    "    print(f\"‚úÖ Las √∫ltimas 98 p√°ginas han sido eliminadas\")\n",
    "    print(f\"‚úÖ Archivo final disponible en: {final_processed_path}\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"=== ERROR EN EL PROCESAMIENTO PASO 2 ===\")\n",
    "    print(\"‚ùå No se pudo completar la eliminaci√≥n de las √∫ltimas p√°ginas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6bc637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICACI√ìN FINAL DEL PROCESAMIENTO COMPLETO ===\n",
      "\n",
      "üìÑ P√°ginas en el archivo final: 402\n",
      "üíæ Tama√±o del archivo final: 1.95 MB\n",
      "üìÅ Ubicaci√≥n final: c:\\Users\\MSI\\Desktop\\DreamTeam\\RAG-Benchmark\\Data\\processed\\guia_embarazo_parto_2023_final.pdf\n",
      "\n",
      "üìä RESUMEN COMPLETO DEL PROCESAMIENTO:\n",
      "   üìÑ Archivo original:\n",
      "      ‚Ä¢ P√°ginas: 623\n",
      "      ‚Ä¢ Tama√±o: 3.54 MB\n",
      "\n",
      "   üóëÔ∏è P√°ginas eliminadas:\n",
      "      ‚Ä¢ Primeras p√°ginas eliminadas: 123\n",
      "      ‚Ä¢ √öltimas p√°ginas eliminadas: 98\n",
      "      ‚Ä¢ Total eliminadas: 221 p√°ginas\n",
      "\n",
      "   üìÑ Archivo final:\n",
      "      ‚Ä¢ P√°ginas: 402\n",
      "      ‚Ä¢ Tama√±o: 1.95 MB\n",
      "      ‚Ä¢ Reducci√≥n de tama√±o: 1.59 MB\n",
      "\n",
      "   ‚úÖ Verificaci√≥n:\n",
      "      ‚Ä¢ P√°ginas esperadas: 402\n",
      "      ‚Ä¢ P√°ginas obtenidas: 402\n",
      "      ‚úÖ ¬°PERFECTO! El procesamiento fue exitoso.\n",
      "      ‚úÖ Se eliminaron correctamente las primeras 123 y √∫ltimas 98 p√°ginas.\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n final completa del procesamiento\n",
    "print(\"=== VERIFICACI√ìN FINAL DEL PROCESAMIENTO COMPLETO ===\")\n",
    "print()\n",
    "\n",
    "if final_processed_path.exists():\n",
    "    # Verificar el archivo final\n",
    "    try:\n",
    "        with open(final_processed_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            final_pages = len(pdf_reader.pages)\n",
    "            final_size_mb = final_processed_path.stat().st_size / (1024*1024)\n",
    "            \n",
    "            print(f\"üìÑ P√°ginas en el archivo final: {final_pages}\")\n",
    "            print(f\"üíæ Tama√±o del archivo final: {final_size_mb:.2f} MB\")\n",
    "            print(f\"üìÅ Ubicaci√≥n final: {final_processed_path.absolute()}\")\n",
    "            \n",
    "            # Comparaci√≥n completa de todo el proceso\n",
    "            with open(raw_data_path, 'rb') as original_file:\n",
    "                original_pdf = PyPDF2.PdfReader(original_file)\n",
    "                original_pages = len(original_pdf.pages)\n",
    "                original_size_mb = raw_data_path.stat().st_size / (1024*1024)\n",
    "                \n",
    "                print()\n",
    "                print(f\"üìä RESUMEN COMPLETO DEL PROCESAMIENTO:\")\n",
    "                print(f\"   üìÑ Archivo original:\")\n",
    "                print(f\"      ‚Ä¢ P√°ginas: {original_pages}\")\n",
    "                print(f\"      ‚Ä¢ Tama√±o: {original_size_mb:.2f} MB\")\n",
    "                print()\n",
    "                print(f\"   üóëÔ∏è P√°ginas eliminadas:\")\n",
    "                print(f\"      ‚Ä¢ Primeras p√°ginas eliminadas: 123\")\n",
    "                print(f\"      ‚Ä¢ √öltimas p√°ginas eliminadas: 98\")\n",
    "                print(f\"      ‚Ä¢ Total eliminadas: {123 + 98} p√°ginas\")\n",
    "                print()\n",
    "                print(f\"   üìÑ Archivo final:\")\n",
    "                print(f\"      ‚Ä¢ P√°ginas: {final_pages}\")\n",
    "                print(f\"      ‚Ä¢ Tama√±o: {final_size_mb:.2f} MB\")\n",
    "                print(f\"      ‚Ä¢ Reducci√≥n de tama√±o: {original_size_mb - final_size_mb:.2f} MB\")\n",
    "                \n",
    "                expected_final_pages = original_pages - 123 - 98\n",
    "                print()\n",
    "                print(f\"   ‚úÖ Verificaci√≥n:\")\n",
    "                print(f\"      ‚Ä¢ P√°ginas esperadas: {expected_final_pages}\")\n",
    "                print(f\"      ‚Ä¢ P√°ginas obtenidas: {final_pages}\")\n",
    "                \n",
    "                if final_pages == expected_final_pages:\n",
    "                    print(f\"      ‚úÖ ¬°PERFECTO! El procesamiento fue exitoso.\")\n",
    "                    print(f\"      ‚úÖ Se eliminaron correctamente las primeras 123 y √∫ltimas 98 p√°ginas.\")\n",
    "                else:\n",
    "                    print(f\"      ‚ùå Error: El n√∫mero de p√°ginas no coincide con lo esperado.\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al verificar el archivo final: {str(e)}\")\n",
    "else:\n",
    "    print(\"‚ùå El archivo final no existe. El procesamiento no se complet√≥ correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a42bdd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACCI√ìN ESTRUCTURADA POR P√ÅGINA ===\n",
      "\n",
      "‚úÖ PyMuPDF disponible\n",
      "PDF de entrada: processed\\guia_embarazo_parto_2023_final.pdf\n",
      "Archivo estructurado: processed\\guia_embarazo_ESTRUCTURADO.txt\n",
      "‚úÖ PDF encontrado: 1.95 MB\n",
      "üìÑ Total de p√°ginas: 402\n",
      "   Procesadas 50/402 p√°ginas...\n",
      "   Procesadas 100/402 p√°ginas...\n",
      "   Procesadas 150/402 p√°ginas...\n",
      "   Procesadas 200/402 p√°ginas...\n",
      "   Procesadas 250/402 p√°ginas...\n",
      "   Procesadas 300/402 p√°ginas...\n",
      "   Procesadas 350/402 p√°ginas...\n",
      "   Procesadas 400/402 p√°ginas...\n",
      "\n",
      "‚úÖ Archivo estructurado creado exitosamente\n",
      "üìÅ Ubicaci√≥n: processed\\guia_embarazo_ESTRUCTURADO.txt\n",
      "üíæ Tama√±o: 0.79 MB\n",
      "üìÑ P√°ginas procesadas: 402\n",
      "üìä Promedio caracteres por p√°gina: 2019\n"
     ]
    }
   ],
   "source": [
    "# Extracci√≥n estructurada: P√ÅGINA | CONTENIDO\n",
    "print(\"=== EXTRACCI√ìN ESTRUCTURADA POR P√ÅGINA ===\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "    fitz_available = True\n",
    "    print(\"‚úÖ PyMuPDF disponible\")\n",
    "except ImportError:\n",
    "    fitz_available = False\n",
    "    print(\"‚ùå PyMuPDF no disponible\")\n",
    "\n",
    "if fitz_available:\n",
    "    input_pdf_path = Path(\"processed/guia_embarazo_parto_2023_final.pdf\")\n",
    "    structured_text_path = Path(\"processed/guia_embarazo_ESTRUCTURADO.txt\")\n",
    "    \n",
    "    print(f\"PDF de entrada: {input_pdf_path}\")\n",
    "    print(f\"Archivo estructurado: {structured_text_path}\")\n",
    "    \n",
    "    if input_pdf_path.exists():\n",
    "        print(f\"‚úÖ PDF encontrado: {input_pdf_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        try:\n",
    "            doc = fitz.open(str(input_pdf_path))\n",
    "            total_pages = len(doc)\n",
    "            print(f\"üìÑ Total de p√°ginas: {total_pages}\")\n",
    "            \n",
    "            structured_lines = []\n",
    "            \n",
    "            for page_num in range(total_pages):\n",
    "                page = doc.load_page(page_num)\n",
    "                \n",
    "                # Extraer texto de la p√°gina\n",
    "                page_text = page.get_text()\n",
    "                \n",
    "                # Limpiar el texto de la p√°gina\n",
    "                # 1. Eliminar saltos de l√≠nea y espacios m√∫ltiples\n",
    "                clean_page_text = re.sub(r'\\s+', ' ', page_text.strip())\n",
    "                \n",
    "                # 2. Eliminar caracteres problem√°ticos\n",
    "                clean_page_text = clean_page_text.replace('\\u00A0', ' ')  # Espacio no rompible\n",
    "                clean_page_text = clean_page_text.replace('\\u2028', ' ')  # Separador de l√≠nea\n",
    "                clean_page_text = clean_page_text.replace('\\u2029', ' ')  # Separador de p√°rrafo\n",
    "                \n",
    "                # 3. Eliminar patrones comunes no deseados\n",
    "                clean_page_text = re.sub(r'\\+\\d+', '', clean_page_text)  # +1, +2, etc.\n",
    "                clean_page_text = re.sub(r'[-_‚ïê‚îÄ‚ñ¨‚ñ†|‚ñå‚ñê‚îÉ]{3,}', '', clean_page_text)  # L√≠neas\n",
    "                \n",
    "                # 4. Limpiar espacios finales\n",
    "                clean_page_text = re.sub(r'\\s+', ' ', clean_page_text).strip()\n",
    "                \n",
    "                # Solo agregar si la p√°gina tiene contenido significativo\n",
    "                if len(clean_page_text) > 10:  # M√≠nimo 10 caracteres\n",
    "                    # Formato: P√ÅGINA | CONTENIDO\n",
    "                    page_line = f\"{page_num + 1}|{clean_page_text}\"\n",
    "                    structured_lines.append(page_line)\n",
    "                \n",
    "                # Progreso cada 50 p√°ginas\n",
    "                if (page_num + 1) % 50 == 0:\n",
    "                    print(f\"   Procesadas {page_num + 1}/{total_pages} p√°ginas...\")\n",
    "            \n",
    "            doc.close()\n",
    "            \n",
    "            # Guardar archivo estructurado\n",
    "            with open(structured_text_path, 'w', encoding='utf-8') as f:\n",
    "                for line in structured_lines:\n",
    "                    f.write(line + '\\n')\n",
    "            \n",
    "            file_size_mb = structured_text_path.stat().st_size / (1024*1024)\n",
    "            print(f\"\\n‚úÖ Archivo estructurado creado exitosamente\")\n",
    "            print(f\"üìÅ Ubicaci√≥n: {structured_text_path}\")\n",
    "            print(f\"üíæ Tama√±o: {file_size_mb:.2f} MB\")\n",
    "            print(f\"üìÑ P√°ginas procesadas: {len(structured_lines)}\")\n",
    "            print(f\"üìä Promedio caracteres por p√°gina: {sum(len(line.split('|', 1)[1]) for line in structured_lines) // len(structured_lines) if structured_lines else 0}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: PDF no encontrado\")\n",
    "else:\n",
    "    print(\"‚ùå Necesitas instalar PyMuPDF: pip install PyMuPDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a477e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n especializada para limpiar contenido por p√°gina\n",
    "def clean_page_content(text):\n",
    "    \"\"\"\n",
    "    Limpia el contenido de una p√°gina espec√≠fica\n",
    "    \"\"\"\n",
    "    if not text or len(text.strip()) < 5:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Normalizar espacios y saltos de l√≠nea\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "    # 2. Eliminar caracteres Unicode problem√°ticos\n",
    "    text = text.replace('\\u00A0', ' ')  # Espacio no rompible\n",
    "    text = text.replace('\\u2028', ' ')  # Separador de l√≠nea\n",
    "    text = text.replace('\\u2029', ' ')  # Separador de p√°rrafo\n",
    "    text = text.replace('\\u200B', '')   # Espacio de ancho cero\n",
    "    \n",
    "    # 3. Eliminar patrones espec√≠ficos\n",
    "    text = re.sub(r'\\+\\d+\\s*', '', text)  # +1, +2, etc.\n",
    "    text = re.sub(r'\\s+\\+\\d+', '', text)  # espacios antes de +1\n",
    "    text = re.sub(r'[-_‚ïê‚îÄ‚ñ¨‚ñ†|‚ñå‚ñê‚îÉ]{3,}', '', text)  # L√≠neas horizontales/verticales\n",
    "    \n",
    "    # 4. Eliminar n√∫meros de p√°gina al inicio o final\n",
    "    text = re.sub(r'^\\d{1,3}\\s+', '', text)  # N√∫mero al inicio\n",
    "    text = re.sub(r'\\s+\\d{1,3}$', '', text)  # N√∫mero al final\n",
    "    \n",
    "    # 5. Limpiar espacios m√∫ltiples nuevamente\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "884e39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n mejorada para limpiar contenido por p√°gina (VERSI√ìN FINAL)\n",
    "def clean_page_content_final(text):\n",
    "    \"\"\"\n",
    "    Limpia el contenido de una p√°gina eliminando TODOS los elementos no deseados\n",
    "    \"\"\"\n",
    "    if not text or len(text.strip()) < 5:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Normalizar espacios y saltos de l√≠nea\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "    # 2. Eliminar caracteres Unicode problem√°ticos\n",
    "    text = text.replace('\\u00A0', ' ')  # Espacio no rompible\n",
    "    text = text.replace('\\u2028', ' ')  # Separador de l√≠nea\n",
    "    text = text.replace('\\u2029', ' ')  # Separador de p√°rrafo\n",
    "    text = text.replace('\\u200B', '')   # Espacio de ancho cero\n",
    "    \n",
    "    # 3. Eliminar patrones espec√≠ficos\n",
    "    text = re.sub(r'\\+\\d+\\s*', '', text)  # +1, +2, etc.\n",
    "    text = re.sub(r'\\s+\\+\\d+', '', text)  # espacios antes de +1\n",
    "    text = re.sub(r'[-_‚ïê‚îÄ‚ñ¨‚ñ†|‚ñå‚ñê‚îÉ]{3,}', '', text)  # L√≠neas horizontales/verticales\n",
    "    \n",
    "    # 4. NUEVO: Eliminar marcadores de evidencia\n",
    "    text = re.sub(r'\\b\\d+\\+\\+?\\b', '', text)  # 1+, 2+, 1++, 2++, etc.\n",
    "    text = re.sub(r'\\b[IVX]+\\+?\\b', '', text)  # I+, II, III, IV, etc.\n",
    "    text = re.sub(r'\\bII\\s+III\\b', '', text)  # \"II III\" espec√≠fico\n",
    "    text = re.sub(r'\\b[IVX]+\\s+[IVX]+\\b', '', text)  # N√∫meros romanos m√∫ltiples\n",
    "    \n",
    "    # 5. NUEVO: Eliminar s√≠mbolos especiales innecesarios\n",
    "    text = text.replace('‚àö', '')  # S√≠mbolo de check\n",
    "    text = text.replace('(‚Üë)', '')  # Flecha hacia arriba\n",
    "    text = text.replace('‚Üë', '')  # Flecha hacia arriba sola\n",
    "    \n",
    "    # 6. Eliminar n√∫meros de p√°gina al inicio o final\n",
    "    text = re.sub(r'^\\d{1,3}\\s+', '', text)  # N√∫mero al inicio\n",
    "    text = re.sub(r'\\s+\\d{1,3}$', '', text)  # N√∫mero al final\n",
    "    \n",
    "    # 7. Eliminar patrones de referencias bibliogr√°ficas sueltas\n",
    "    text = re.sub(r'\\(\\d{1,4}\\)', '', text)  # (123), (45), etc. - referencias\n",
    "    \n",
    "    # 8. Limpiar espacios m√∫ltiples nuevamente\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 9. Eliminar texto residual muy corto al final\n",
    "    if len(text) < 20:\n",
    "        return \"\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6f5577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACCI√ìN FINAL ULTRA-LIMPIA ===\n",
      "\n",
      "üìÑ Procesando 402 p√°ginas...\n",
      "   ‚úì 50/402 p√°ginas\n",
      "   ‚úì 100/402 p√°ginas\n",
      "   ‚úì 150/402 p√°ginas\n",
      "   ‚úì 200/402 p√°ginas\n",
      "   ‚úì 250/402 p√°ginas\n",
      "   ‚úì 300/402 p√°ginas\n",
      "   ‚úì 350/402 p√°ginas\n",
      "   ‚úì 400/402 p√°ginas\n",
      "\n",
      "üéâ ¬°ARCHIVO ULTRA-LIMPIO LISTO!\n",
      "üìÅ Archivo: processed\\guia_embarazo_ULTRA_CLEAN.txt\n",
      "üíæ Tama√±o: 0.79 MB\n",
      "üìÑ P√°ginas procesadas: 402\n",
      "üßπ Limpieza aplicada:\n",
      "   ‚úÖ Sin marcadores de evidencia (1+, 2+, II III)\n",
      "   ‚úÖ Sin s√≠mbolos especiales (‚àö, ‚Üë)\n",
      "   ‚úÖ Sin referencias bibliogr√°ficas sueltas\n",
      "   ‚úÖ Sin l√≠neas horizontales\n",
      "   ‚úÖ Formato: P√ÅGINA|CONTENIDO_ULTRA_LIMPIO\n",
      "\n",
      "üéØ ESTE ES EL ARCHIVO FINAL PARA CHUNKEO\n"
     ]
    }
   ],
   "source": [
    "# EXTRACCI√ìN FINAL MEJORADA: Sin marcadores de evidencia\n",
    "print(\"=== EXTRACCI√ìN FINAL ULTRA-LIMPIA ===\")\n",
    "print()\n",
    "\n",
    "def extract_ultra_clean_text(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Extrae texto por p√°gina con limpieza COMPLETA\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import fitz\n",
    "        \n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        total_pages = len(doc)\n",
    "        print(f\"üìÑ Procesando {total_pages} p√°ginas...\")\n",
    "        \n",
    "        structured_lines = []\n",
    "        \n",
    "        for page_num in range(total_pages):\n",
    "            page = doc.load_page(page_num)\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            # Aplicar limpieza COMPLETA\n",
    "            clean_content = clean_page_content_final(page_text)\n",
    "            \n",
    "            # Solo agregar p√°ginas con contenido significativo\n",
    "            if len(clean_content) > 30:  # Aumentamos el m√≠nimo a 30 caracteres\n",
    "                page_line = f\"{page_num + 1}|{clean_content}\"\n",
    "                structured_lines.append(page_line)\n",
    "            \n",
    "            if (page_num + 1) % 50 == 0:\n",
    "                print(f\"   ‚úì {page_num + 1}/{total_pages} p√°ginas\")\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Guardar archivo ultra-limpio\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for line in structured_lines:\n",
    "                f.write(line + '\\n')\n",
    "        \n",
    "        return len(structured_lines)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "# Ejecutar extracci√≥n ultra-limpia\n",
    "input_pdf_path = Path(\"processed/guia_embarazo_parto_2023_final.pdf\")\n",
    "\n",
    "if input_pdf_path.exists():\n",
    "    ultra_clean_path = Path(\"processed/guia_embarazo_ULTRA_CLEAN.txt\")\n",
    "    \n",
    "    pages_processed = extract_ultra_clean_text(input_pdf_path, ultra_clean_path)\n",
    "    \n",
    "    if pages_processed > 0:\n",
    "        file_size_mb = ultra_clean_path.stat().st_size / (1024*1024)\n",
    "        print(f\"\\nüéâ ¬°ARCHIVO ULTRA-LIMPIO LISTO!\")\n",
    "        print(f\"üìÅ Archivo: {ultra_clean_path}\")\n",
    "        print(f\"üíæ Tama√±o: {file_size_mb:.2f} MB\")\n",
    "        print(f\"üìÑ P√°ginas procesadas: {pages_processed}\")\n",
    "        print(f\"üßπ Limpieza aplicada:\")\n",
    "        print(f\"   ‚úÖ Sin marcadores de evidencia (1+, 2+, II III)\")\n",
    "        print(f\"   ‚úÖ Sin s√≠mbolos especiales (‚àö, ‚Üë)\")\n",
    "        print(f\"   ‚úÖ Sin referencias bibliogr√°ficas sueltas\")\n",
    "        print(f\"   ‚úÖ Sin l√≠neas horizontales\")\n",
    "        print(f\"   ‚úÖ Formato: P√ÅGINA|CONTENIDO_ULTRA_LIMPIO\")\n",
    "        print(f\"\\nüéØ ESTE ES EL ARCHIVO FINAL PARA CHUNKEO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42a86b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHUNKING MEJORADO CON METADATOS DE SECCIONES ===\n",
      "\n",
      "üöÄ Iniciando proceso de chunking MEJORADO...\n",
      "\n",
      "üìÅ Archivo de entrada: processed\\guia_embarazo_ULTRA_CLEAN.txt\n",
      "üìÅ Archivo de salida: chunks\\chunks_with_sections_20250826_114329.json\n",
      "üìä Configuraci√≥n: 500 caracteres, 100 overlap\n",
      "\n",
      "üîç Detectando secciones del documento...\n",
      "   üìç Secci√≥n 1 detectada en p√°gina 1\n",
      "   üìç Secci√≥n 2 detectada en p√°gina 146\n",
      "   üìç Secci√≥n 3 detectada en p√°gina 202\n",
      "   üìç Secci√≥n 4 detectada en p√°gina 248\n",
      "   üìç Secci√≥n 5 detectada en p√°gina 278\n",
      "   üìç Secci√≥n 6 detectada en p√°gina 358\n",
      "‚úÖ Secciones detectadas: {1: 1, 2: 146, 3: 202, 4: 248, 5: 278, 6: 358}\n",
      "\n",
      "üìÑ Total de p√°ginas en el archivo: 402\n",
      "   ‚úì Procesadas 50/402 p√°ginas...\n",
      "   ‚úì Procesadas 100/402 p√°ginas...\n",
      "   ‚úì Procesadas 150/402 p√°ginas...\n",
      "   ‚úì Procesadas 200/402 p√°ginas...\n",
      "   ‚úì Procesadas 250/402 p√°ginas...\n",
      "   ‚úì Procesadas 300/402 p√°ginas...\n",
      "   ‚úì Procesadas 350/402 p√°ginas...\n",
      "   ‚úì Procesadas 400/402 p√°ginas...\n",
      "\n",
      "üéâ ¬°CHUNKING MEJORADO COMPLETADO EXITOSAMENTE!\n",
      "üìÅ Archivo de chunks: chunks\\chunks_with_sections_20250826_114329.json\n",
      "üíæ Tama√±o del archivo: 2.20 MB\n",
      "\n",
      "üìä ESTAD√çSTICAS GENERALES:\n",
      "   üìÑ P√°ginas procesadas: 402\n",
      "   üß© Total de chunks creados: 2361\n",
      "   üìù Total de caracteres: 1,001,695\n",
      "   üî§ Total de palabras: 160,031\n",
      "   üìè Promedio caracteres por chunk: 424\n",
      "   üìè Promedio palabras por chunk: 67\n",
      "   üîÑ Overlap configurado: 100 caracteres\n",
      "\n",
      "üìö ESTAD√çSTICAS POR SECCI√ìN:\n",
      "   üìñ Secci√≥n 1: PREVENCI√ìN Y DETECCI√ìN TEMPRANA DE LAS ALTERACIONE...\n",
      "      ‚Ä¢ Chunks: 851\n",
      "      ‚Ä¢ P√°ginas: 145\n",
      "      ‚Ä¢ Caracteres: 361,518\n",
      "      ‚Ä¢ Palabras: 58,737\n",
      "   üìñ Secci√≥n 2: ABORDAJE DE LAS COMPLICACIONES HIPERTENSIVAS ASOCI...\n",
      "      ‚Ä¢ Chunks: 330\n",
      "      ‚Ä¢ P√°ginas: 56\n",
      "      ‚Ä¢ Caracteres: 137,177\n",
      "      ‚Ä¢ Palabras: 21,190\n",
      "   üìñ Secci√≥n 3: INFECCIONES EN EL EMBARAZO: RUPTURA PREMATURA DE M...\n",
      "      ‚Ä¢ Chunks: 266\n",
      "      ‚Ä¢ P√°ginas: 46\n",
      "      ‚Ä¢ Caracteres: 113,417\n",
      "      ‚Ä¢ Palabras: 18,125\n",
      "   üìñ Secci√≥n 4: NUTRICI√ìN EN EL EMBARAZO...\n",
      "      ‚Ä¢ Chunks: 177\n",
      "      ‚Ä¢ P√°ginas: 30\n",
      "      ‚Ä¢ Caracteres: 73,745\n",
      "      ‚Ä¢ Palabras: 11,623\n",
      "   üìñ Secci√≥n 5: DETECCI√ìN TEMPRANA DE LAS ANOMAL√çAS DURANTE EL TRA...\n",
      "      ‚Ä¢ Chunks: 481\n",
      "      ‚Ä¢ P√°ginas: 80\n",
      "      ‚Ä¢ Caracteres: 205,591\n",
      "      ‚Ä¢ Palabras: 32,801\n",
      "   üìñ Secci√≥n 6: COMPLICACIONES HEMORR√ÅGICAS ASOCIADAS AL EMBARAZO...\n",
      "      ‚Ä¢ Chunks: 256\n",
      "      ‚Ä¢ P√°ginas: 45\n",
      "      ‚Ä¢ Caracteres: 110,247\n",
      "      ‚Ä¢ Palabras: 17,555\n",
      "\n",
      "üéØ CHUNKS CON METADATOS COMPLETOS LISTOS PARA RAG!\n",
      "\n",
      "üìã EJEMPLOS DE CHUNKS CON METADATOS:\n",
      "\n",
      "   üìÑ Chunk 1:\n",
      "   ID: sec1_page1_chunk1\n",
      "   P√°gina: 1\n",
      "   Secci√≥n: 1 - PREVENCI√ìN Y DETECCI√ìN TEMPRANA DE LAS A...\n",
      "   Caracteres: 446\n",
      "   Dominio: medicina_obstetrica\n",
      "   Contenido: SECCI√ìN 1. PREVENCI√ìN Y DETECCI√ìN TEMPRANA DE LAS ALTERACIONES DEL EMBARAZO. PRE...\n",
      "\n",
      "   üìÑ Chunk 2:\n",
      "   ID: sec1_page1_chunk2\n",
      "   P√°gina: 1\n",
      "   Secci√≥n: 1 - PREVENCI√ìN Y DETECCI√ìN TEMPRANA DE LAS A...\n",
      "   Caracteres: 500\n",
      "   Dominio: medicina_obstetrica\n",
      "   Contenido: ormal? 4. ¬øQu√© registros documentales se deben diligenciar durante las citas de ...\n",
      "\n",
      "   ... y 2359 chunks m√°s\n"
     ]
    }
   ],
   "source": [
    "# PASO 4 MEJORADO: CHUNKING CON METADATOS DE SECCIONES\n",
    "print(\"=== CHUNKING MEJORADO CON METADATOS DE SECCIONES ===\")\n",
    "print()\n",
    "\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Definir las 6 secciones del documento\n",
    "SECTIONS = {\n",
    "    1: {\n",
    "        \"title\": \"PREVENCI√ìN Y DETECCI√ìN TEMPRANA DE LAS ALTERACIONES DEL EMBARAZO\",\n",
    "        \"start_page\": 1,\n",
    "        \"end_page\": 145  # Aproximado, se ajustar√° din√°micamente\n",
    "    },\n",
    "    2: {\n",
    "        \"title\": \"ABORDAJE DE LAS COMPLICACIONES HIPERTENSIVAS ASOCIADAS AL EMBARAZO\", \n",
    "        \"start_page\": 146,\n",
    "        \"end_page\": 201\n",
    "    },\n",
    "    3: {\n",
    "        \"title\": \"INFECCIONES EN EL EMBARAZO: RUPTURA PREMATURA DE MEMBRANAS (RPM)\",\n",
    "        \"start_page\": 202,\n",
    "        \"end_page\": 277\n",
    "    },\n",
    "    4: {\n",
    "        \"title\": \"NUTRICI√ìN EN EL EMBARAZO\",\n",
    "        \"start_page\": 278,  # Se ajustar√° din√°micamente\n",
    "        \"end_page\": 350   # Aproximado\n",
    "    },\n",
    "    5: {\n",
    "        \"title\": \"DETECCI√ìN TEMPRANA DE LAS ANOMAL√çAS DURANTE EL TRABAJO DE PARTO, ATENCI√ìN DEL PARTO NORMAL Y DIST√ìCICO\",\n",
    "        \"start_page\": 278,\n",
    "        \"end_page\": 357\n",
    "    },\n",
    "    6: {\n",
    "        \"title\": \"COMPLICACIONES HEMORR√ÅGICAS ASOCIADAS AL EMBARAZO\",\n",
    "        \"start_page\": 358,\n",
    "        \"end_page\": 450  # Hasta el final\n",
    "    }\n",
    "}\n",
    "\n",
    "def detect_section_boundaries(input_file_path):\n",
    "    \"\"\"\n",
    "    Detecta autom√°ticamente los l√≠mites de las secciones bas√°ndose en el contenido\n",
    "    \"\"\"\n",
    "    section_pages = {}\n",
    "    \n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if '|' in line:\n",
    "                page_num_str, content = line.split('|', 1)\n",
    "                page_num = int(page_num_str.strip())\n",
    "                content = content.strip()\n",
    "                \n",
    "                # Buscar marcadores de secci√≥n\n",
    "                if re.search(r'SECCI√ìN\\s+(\\d+)', content, re.IGNORECASE):\n",
    "                    section_match = re.search(r'SECCI√ìN\\s+(\\d+)', content, re.IGNORECASE)\n",
    "                    section_number = int(section_match.group(1))\n",
    "                    section_pages[section_number] = page_num\n",
    "                    print(f\"   üìç Secci√≥n {section_number} detectada en p√°gina {page_num}\")\n",
    "        \n",
    "        return section_pages\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error detectando secciones: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def get_section_for_page(page_num, section_boundaries):\n",
    "    \"\"\"\n",
    "    Determina a qu√© secci√≥n pertenece una p√°gina dada\n",
    "    \"\"\"\n",
    "    # Ordenar las secciones por n√∫mero\n",
    "    sorted_sections = sorted(section_boundaries.items())\n",
    "    \n",
    "    # Encontrar la secci√≥n correcta\n",
    "    for i, (section_num, start_page) in enumerate(sorted_sections):\n",
    "        # Si es la √∫ltima secci√≥n, va hasta el final\n",
    "        if i == len(sorted_sections) - 1:\n",
    "            if page_num >= start_page:\n",
    "                return section_num\n",
    "        else:\n",
    "            # Obtener la p√°gina de inicio de la siguiente secci√≥n\n",
    "            next_start_page = sorted_sections[i + 1][1]\n",
    "            if start_page <= page_num < next_start_page:\n",
    "                return section_num\n",
    "    \n",
    "    # Si no se encuentra, asumir secci√≥n 1\n",
    "    return 1\n",
    "\n",
    "def chunk_text_with_overlap_improved(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"\n",
    "    Divide el texto en chunks con overlap espec√≠fico (versi√≥n mejorada)\n",
    "    \"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Calcular el final del chunk\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        # Si llegamos al final del texto, tomar todo lo que queda\n",
    "        if end >= len(text):\n",
    "            chunk = text[start:]\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk.strip())\n",
    "            break\n",
    "        \n",
    "        # Extraer el chunk\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Intentar terminar en un espacio, punto o coma para no cortar palabras/oraciones\n",
    "        if end < len(text) and text[end] not in [' ', '.', ',', ';', ':', '!', '?']:\n",
    "            # Buscar hacia atr√°s el √∫ltimo separador de oraci√≥n\n",
    "            last_sentence_end = max(\n",
    "                chunk.rfind('.'), chunk.rfind('!'), chunk.rfind('?'),\n",
    "                chunk.rfind(';')\n",
    "            )\n",
    "            if last_sentence_end > chunk_size * 0.6:  # Si est√° en el √∫ltimo 40%\n",
    "                chunk = chunk[:last_sentence_end + 1]\n",
    "                end = start + last_sentence_end + 1\n",
    "            else:\n",
    "                # Si no hay separador de oraci√≥n, buscar espacio\n",
    "                last_space = chunk.rfind(' ')\n",
    "                if last_space > chunk_size * 0.7:  # Si est√° en el √∫ltimo 30%\n",
    "                    chunk = chunk[:last_space]\n",
    "                    end = start + last_space\n",
    "        \n",
    "        chunks.append(chunk.strip())\n",
    "        \n",
    "        # Calcular el siguiente punto de inicio con overlap\n",
    "        start = end - overlap\n",
    "        \n",
    "        # Asegurar que no retrocedamos demasiado\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_file_chunking_with_sections(input_file_path, output_file_path, chunk_size=500, overlap=100):\n",
    "    \"\"\"\n",
    "    Procesa el archivo estructurado y crea chunks con metadatos completos incluyendo secciones\n",
    "    \"\"\"\n",
    "    print(f\"üìÅ Archivo de entrada: {input_file_path}\")\n",
    "    print(f\"üìÅ Archivo de salida: {output_file_path}\")\n",
    "    print(f\"üìä Configuraci√≥n: {chunk_size} caracteres, {overlap} overlap\")\n",
    "    print()\n",
    "    \n",
    "    # Detectar l√≠mites de secciones\n",
    "    print(\"üîç Detectando secciones del documento...\")\n",
    "    section_boundaries = detect_section_boundaries(input_file_path)\n",
    "    \n",
    "    if not section_boundaries:\n",
    "        print(\"‚ö†Ô∏è No se pudieron detectar secciones, usando valores por defecto\")\n",
    "        section_boundaries = {1: 1, 2: 146, 3: 202, 4: 278, 5: 278, 6: 358}\n",
    "    \n",
    "    print(f\"‚úÖ Secciones detectadas: {section_boundaries}\")\n",
    "    print()\n",
    "    \n",
    "    chunks_data = []\n",
    "    \n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        total_lines = len(lines)\n",
    "        print(f\"üìÑ Total de p√°ginas en el archivo: {total_lines}\")\n",
    "        \n",
    "        for line_idx, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Separar p√°gina y contenido\n",
    "            if '|' in line:\n",
    "                page_num_str, content = line.split('|', 1)\n",
    "                page_num = int(page_num_str.strip())\n",
    "                content = content.strip()\n",
    "                \n",
    "                if len(content) < 20:  # Saltar p√°ginas con muy poco contenido\n",
    "                    continue\n",
    "                \n",
    "                # Determinar la secci√≥n\n",
    "                section_number = get_section_for_page(page_num, section_boundaries)\n",
    "                section_info = SECTIONS.get(section_number, SECTIONS[1])\n",
    "                \n",
    "                # Crear chunks del contenido de esta p√°gina\n",
    "                page_chunks = chunk_text_with_overlap_improved(content, chunk_size, overlap)\n",
    "                \n",
    "                # Crear metadatos para cada chunk\n",
    "                for chunk_idx, chunk_text in enumerate(page_chunks):\n",
    "                    chunk_data = {\n",
    "                        \"chunk_id\": f\"sec{section_number}_page{page_num}_chunk{chunk_idx + 1}\",\n",
    "                        \"page_number\": page_num,\n",
    "                        \"chunk_index\": chunk_idx + 1,\n",
    "                        \"total_chunks_in_page\": len(page_chunks),\n",
    "                        \"content\": chunk_text,\n",
    "                        \"char_count\": len(chunk_text),\n",
    "                        \"word_count\": len(chunk_text.split()),\n",
    "                        \"source_file\": \"guia_embarazo_parto_2023_final.pdf\",\n",
    "                        \n",
    "                        # NUEVOS METADATOS DE SECCI√ìN\n",
    "                        \"section_number\": section_number,\n",
    "                        \"section_title\": section_info[\"title\"],\n",
    "                        \"section_topic\": \"embarazo_parto\",  # Tema general\n",
    "                        \n",
    "                        # Metadatos adicionales √∫tiles para RAG\n",
    "                        \"document_type\": \"guia_clinica\",\n",
    "                        \"language\": \"es\",\n",
    "                        \"domain\": \"medicina_obstetrica\",\n",
    "                        \"chunk_quality\": \"high\" if len(chunk_text) > 100 else \"medium\"\n",
    "                    }\n",
    "                    chunks_data.append(chunk_data)\n",
    "            \n",
    "            # Progreso cada 50 p√°ginas\n",
    "            if (line_idx + 1) % 50 == 0:\n",
    "                print(f\"   ‚úì Procesadas {line_idx + 1}/{total_lines} p√°ginas...\")\n",
    "        \n",
    "        # Guardar chunks como JSON\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        return chunks_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Ejecutar el chunking mejorado\n",
    "input_text_file = Path(\"processed/guia_embarazo_ULTRA_CLEAN.txt\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_chunks_file = Path(f\"chunks/chunks_with_sections_{timestamp}.json\")\n",
    "\n",
    "# Crear directorio chunks si no existe\n",
    "output_chunks_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if input_text_file.exists():\n",
    "    print(\"üöÄ Iniciando proceso de chunking MEJORADO...\")\n",
    "    print()\n",
    "    \n",
    "    chunks = process_file_chunking_with_sections(\n",
    "        input_text_file, \n",
    "        output_chunks_file, \n",
    "        chunk_size=500, \n",
    "        overlap=100\n",
    "    )\n",
    "    \n",
    "    if chunks:\n",
    "        file_size_mb = output_chunks_file.stat().st_size / (1024*1024)\n",
    "        \n",
    "        # Estad√≠sticas generales\n",
    "        total_chunks = len(chunks)\n",
    "        total_chars = sum(chunk['char_count'] for chunk in chunks)\n",
    "        total_words = sum(chunk['word_count'] for chunk in chunks)\n",
    "        avg_chars = total_chars // total_chunks if total_chunks > 0 else 0\n",
    "        avg_words = total_words // total_chunks if total_chunks > 0 else 0\n",
    "        \n",
    "        pages_with_chunks = len(set(chunk['page_number'] for chunk in chunks))\n",
    "        \n",
    "        # Estad√≠sticas por secci√≥n\n",
    "        section_stats = {}\n",
    "        for chunk in chunks:\n",
    "            section_num = chunk['section_number']\n",
    "            if section_num not in section_stats:\n",
    "                section_stats[section_num] = {\n",
    "                    'title': chunk['section_title'],\n",
    "                    'chunks': 0,\n",
    "                    'pages': set(),\n",
    "                    'chars': 0,\n",
    "                    'words': 0\n",
    "                }\n",
    "            section_stats[section_num]['chunks'] += 1\n",
    "            section_stats[section_num]['pages'].add(chunk['page_number'])\n",
    "            section_stats[section_num]['chars'] += chunk['char_count']\n",
    "            section_stats[section_num]['words'] += chunk['word_count']\n",
    "        \n",
    "        print(f\"\\nüéâ ¬°CHUNKING MEJORADO COMPLETADO EXITOSAMENTE!\")\n",
    "        print(f\"üìÅ Archivo de chunks: {output_chunks_file}\")\n",
    "        print(f\"üíæ Tama√±o del archivo: {file_size_mb:.2f} MB\")\n",
    "        print()\n",
    "        print(f\"üìä ESTAD√çSTICAS GENERALES:\")\n",
    "        print(f\"   üìÑ P√°ginas procesadas: {pages_with_chunks}\")\n",
    "        print(f\"   üß© Total de chunks creados: {total_chunks}\")\n",
    "        print(f\"   üìù Total de caracteres: {total_chars:,}\")\n",
    "        print(f\"   üî§ Total de palabras: {total_words:,}\")\n",
    "        print(f\"   üìè Promedio caracteres por chunk: {avg_chars}\")\n",
    "        print(f\"   üìè Promedio palabras por chunk: {avg_words}\")\n",
    "        print(f\"   üîÑ Overlap configurado: 100 caracteres\")\n",
    "        print()\n",
    "        print(f\"üìö ESTAD√çSTICAS POR SECCI√ìN:\")\n",
    "        for section_num in sorted(section_stats.keys()):\n",
    "            stats = section_stats[section_num]\n",
    "            print(f\"   üìñ Secci√≥n {section_num}: {stats['title'][:50]}...\")\n",
    "            print(f\"      ‚Ä¢ Chunks: {stats['chunks']}\")\n",
    "            print(f\"      ‚Ä¢ P√°ginas: {len(stats['pages'])}\")\n",
    "            print(f\"      ‚Ä¢ Caracteres: {stats['chars']:,}\")\n",
    "            print(f\"      ‚Ä¢ Palabras: {stats['words']:,}\")\n",
    "        print()\n",
    "        print(f\"üéØ CHUNKS CON METADATOS COMPLETOS LISTOS PARA RAG!\")\n",
    "        \n",
    "        # Mostrar algunos ejemplos\n",
    "        print(f\"\\nüìã EJEMPLOS DE CHUNKS CON METADATOS:\")\n",
    "        for i, chunk in enumerate(chunks[:2]):\n",
    "            print(f\"\\n   üìÑ Chunk {i+1}:\")\n",
    "            print(f\"   ID: {chunk['chunk_id']}\")\n",
    "            print(f\"   P√°gina: {chunk['page_number']}\")\n",
    "            print(f\"   Secci√≥n: {chunk['section_number']} - {chunk['section_title'][:40]}...\")\n",
    "            print(f\"   Caracteres: {chunk['char_count']}\")\n",
    "            print(f\"   Dominio: {chunk['domain']}\")\n",
    "            print(f\"   Contenido: {chunk['content'][:80]}...\")\n",
    "        \n",
    "        if len(chunks) > 2:\n",
    "            print(f\"\\n   ... y {len(chunks) - 2} chunks m√°s\")\n",
    "    else:\n",
    "        print(\"‚ùå No se pudieron crear los chunks\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: Archivo no encontrado: {input_text_file}\")\n",
    "    print(\"   Primero ejecuta las celdas anteriores para crear el texto ultra-limpio\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
