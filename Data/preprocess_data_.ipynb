{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a03e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPROCESAMIENTO SIMPLIFICADO ===\n",
      "üìÅ PDF original: raw\\guia_embarazo_parto_2023.pdf\n",
      "üìÅ PDF limpio: processed\\guia_embarazo_parto_clean.pdf\n",
      "üìÅ Texto limpio: processed\\guia_embarazo_clean.txt\n",
      "üìÅ Chunks finales: chunks\\chunks_final.json\n",
      "\n",
      "‚úÖ Archivo original encontrado (3.54 MB)\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento simplificado de datos - Gu√≠a de Embarazo y Parto 2023\n",
    "# Genera directamente los archivos finales necesarios\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuraci√≥n de rutas\n",
    "raw_pdf_path = Path(\"raw/guia_embarazo_parto_2023.pdf\")\n",
    "clean_pdf_path = Path(\"processed/guia_embarazo_parto_clean.pdf\")\n",
    "clean_text_path = Path(\"processed/guia_embarazo_clean.txt\")\n",
    "chunks_path = Path(\"chunks/chunks_final.json\")\n",
    "\n",
    "# Crear directorios si no existen\n",
    "clean_pdf_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "clean_text_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "chunks_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=== PREPROCESAMIENTO SIMPLIFICADO ===\")\n",
    "print(f\"üìÅ PDF original: {raw_pdf_path}\")\n",
    "print(f\"üìÅ PDF limpio: {clean_pdf_path}\")\n",
    "print(f\"üìÅ Texto limpio: {clean_text_path}\")\n",
    "print(f\"üìÅ Chunks finales: {chunks_path}\")\n",
    "print()\n",
    "\n",
    "# Verificar archivo original\n",
    "if raw_pdf_path.exists():\n",
    "    size_mb = raw_pdf_path.stat().st_size / (1024*1024)\n",
    "    print(f\"‚úÖ Archivo original encontrado ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: No se encuentra {raw_pdf_path}\")\n",
    "    print(\"   Coloca el archivo PDF original en la carpeta 'raw/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db475bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pdf_direct(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Limpia el PDF directamente: \n",
    "    - Elimina primeras 123 p√°ginas \n",
    "    - Elimina √∫ltimas 98 p√°ginas\n",
    "    - Elimina p√°gina 401 espec√≠ficamente (RECOMENDACIONES PARA LA IMPLEMENTACI√ìN)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_path, 'rb') as input_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(input_file)\n",
    "            pdf_writer = PyPDF2.PdfWriter()\n",
    "            \n",
    "            total_pages = len(pdf_reader.pages)\n",
    "            print(f\"üìÑ P√°ginas originales: {total_pages}\")\n",
    "            \n",
    "            # Calcular p√°ginas a mantener\n",
    "            start_page = 123  # Eliminar primeras 123 p√°ginas (√≠ndice 0-122)\n",
    "            end_page = total_pages - 98  # Eliminar √∫ltimas 98 p√°ginas\n",
    "            \n",
    "            print(f\"üóëÔ∏è Eliminando p√°ginas 1-{start_page} (primeras)\")\n",
    "            print(f\"üóëÔ∏è Eliminando p√°ginas {end_page + 1}-{total_pages} (√∫ltimas)\")\n",
    "            \n",
    "            # Eliminar p√°gina 401 espec√≠ficamente\n",
    "            # En el PDF original, despu√©s de eliminar las primeras 123 p√°ginas,\n",
    "            # la p√°gina 401 del visualizador ser√° el √≠ndice 401-1=400 en base 0\n",
    "            # Pero ajustado por eliminar las primeras 123, ser√° 400-123=277\n",
    "            exclude_page_adjusted = 401 - 123 - 1  # = 277 (√≠ndice 0-based despu√©s del ajuste)\n",
    "            print(f\"üóëÔ∏è Eliminando p√°gina 401 (RECOMENDACIONES) = √≠ndice ajustado {exclude_page_adjusted}\")\n",
    "            \n",
    "            # Agregar p√°ginas v√°lidas al PDF limpio\n",
    "            pages_added = 0\n",
    "            for page_num in range(start_page, end_page):\n",
    "                adjusted_index = page_num - start_page\n",
    "                if adjusted_index != exclude_page_adjusted:\n",
    "                    pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "                    pages_added += 1\n",
    "                else:\n",
    "                    print(f\"üóëÔ∏è Saltando p√°gina 401 (RECOMENDACIONES)\")\n",
    "            \n",
    "            # Guardar PDF limpio\n",
    "            with open(output_path, 'wb') as output_file:\n",
    "                pdf_writer.write(output_file)\n",
    "            \n",
    "            final_size = output_path.stat().st_size / (1024*1024)\n",
    "            print(f\"‚úÖ PDF limpio creado: {pages_added} p√°ginas ({final_size:.2f} MB)\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_text(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Extrae texto limpio del PDF en formato P√ÅGINA|CONTENIDO\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        \n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        total_pages = len(doc)\n",
    "        print(f\"üìÑ Extrayendo texto de {total_pages} p√°ginas...\")\n",
    "        \n",
    "        clean_lines = []\n",
    "        \n",
    "        for page_num in range(total_pages):\n",
    "            page = doc.load_page(page_num)\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            # Limpiar texto de la p√°gina\n",
    "            clean_text = clean_page_content(page_text)\n",
    "            \n",
    "            # Solo agregar p√°ginas con contenido v√°lido\n",
    "            if len(clean_text) > 30:\n",
    "                line = f\"{page_num + 1}|{clean_text}\"\n",
    "                clean_lines.append(line)\n",
    "            \n",
    "            if (page_num + 1) % 50 == 0:\n",
    "                print(f\"   ‚úì {page_num + 1}/{total_pages} p√°ginas procesadas\")\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Guardar texto limpio\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for line in clean_lines:\n",
    "                f.write(line + '\\n')\n",
    "        \n",
    "        file_size = output_path.stat().st_size / (1024*1024)\n",
    "        print(f\"‚úÖ Texto limpio creado: {len(clean_lines)} p√°ginas ({file_size:.2f} MB)\")\n",
    "        return len(clean_lines)\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå Error: Instala PyMuPDF con: pip install PyMuPDF\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def clean_page_content(text):\n",
    "    \"\"\"Limpia el contenido de una p√°gina\"\"\"\n",
    "    if not text or len(text.strip()) < 5:\n",
    "        return \"\"\n",
    "    \n",
    "    # Eliminar letras de recomendaci√≥n aisladas (A, B, C, D, R, V) al inicio de l√≠neas\n",
    "    # Esto es espec√≠fico para las tablas de recomendaciones\n",
    "    text = re.sub(r'^\\s*[A-DRV]\\s+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Normalizar espacios, pero manteniendo saltos de l√≠nea para el procesamiento anterior\n",
    "    text = text.replace('\\u00A0', ' ').replace('\\u2028', ' ').replace('\\u2029', ' ')\n",
    "    \n",
    "    # Eliminar marcadores de evidencia y s√≠mbolos\n",
    "    # Formatos: 1++, 1+, 1-, 2++, 2+, 2-, 3, 4\n",
    "    text = re.sub(r'\\b\\d[+\\-]{1,2}\\b', '', text)\n",
    "    # Formatos: Ia, Ib, II, III, IV\n",
    "    text = re.sub(r'\\b(Ia|Ib|II|III|IV)\\b', '', text)\n",
    "    # Formatos num√©ricos y con cruces que quedaron\n",
    "    text = re.sub(r'\\+\\d+\\s*', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\+\\+?\\b', '', text)\n",
    "    text = re.sub(r'\\b[IVX]+\\+?\\b', '', text)\n",
    "    text = re.sub(r'[-_‚ïê‚îÄ‚ñ¨‚ñ†|‚ñå‚ñê‚îÉ]{3,}', '', text)\n",
    "    text = text.replace('‚àö', '').replace('‚Üë', '')\n",
    "    \n",
    "    # Eliminar n√∫meros de p√°gina y referencias entre par√©ntesis\n",
    "    text = re.sub(r'^\\d{1,3}\\s+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+\\d{1,3}$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\(\\d{1,4}\\)', '', text)\n",
    "    \n",
    "    # Finalmente, colapsar todos los espacios m√∫ltiples a uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be5c4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text_file_path, output_path, chunk_size=500, overlap=100):\n",
    "    \"\"\"\n",
    "    Crea chunks directamente del archivo de texto limpio con section_number y section_title\n",
    "    \"\"\"\n",
    "    # Definir secciones del documento\n",
    "    SECTIONS = {\n",
    "        1: {\n",
    "            \"title\": \"PREVENCI√ìN Y DETECCI√ìN TEMPRANA DE LAS ALTERACIONES DEL EMBARAZO\",\n",
    "            \"start_page\": 1,\n",
    "            \"end_page\": 145\n",
    "        },\n",
    "        2: {\n",
    "            \"title\": \"ABORDAJE DE LAS COMPLICACIONES HIPERTENSIVAS ASOCIADAS AL EMBARAZO\", \n",
    "            \"start_page\": 146,\n",
    "            \"end_page\": 201\n",
    "        },\n",
    "        3: {\n",
    "            \"title\": \"INFECCIONES EN EL EMBARAZO: RUPTURA PREMATURA DE MEMBRANAS (RPM)\",\n",
    "            \"start_page\": 202,\n",
    "            \"end_page\": 277\n",
    "        },\n",
    "        4: {\n",
    "            \"title\": \"NUTRICI√ìN EN EL EMBARAZO\",\n",
    "            \"start_page\": 278,\n",
    "            \"end_page\": 310\n",
    "        },\n",
    "        5: {\n",
    "            \"title\": \"DETECCI√ìN TEMPRANA DE LAS ANOMAL√çAS DURANTE EL TRABAJO DE PARTO, ATENCI√ìN DEL PARTO NORMAL Y DIST√ìCICO\",\n",
    "            \"start_page\": 311,\n",
    "            \"end_page\": 357\n",
    "        },\n",
    "        6: {\n",
    "            \"title\": \"COMPLICACIONES HEMORR√ÅGICAS ASOCIADAS AL EMBARAZO\",\n",
    "            \"start_page\": 358,\n",
    "            \"end_page\": 400\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def get_section_for_page(page_num):\n",
    "        \"\"\"Determina a qu√© secci√≥n pertenece una p√°gina\"\"\"\n",
    "        for section_num, section_info in SECTIONS.items():\n",
    "            if section_info[\"start_page\"] <= page_num <= section_info[\"end_page\"]:\n",
    "                return section_num, section_info[\"title\"]\n",
    "        return 1, SECTIONS[1][\"title\"]  # Por defecto secci√≥n 1\n",
    "    \n",
    "    try:\n",
    "        with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        chunks_data = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or '|' not in line:\n",
    "                continue\n",
    "            \n",
    "            page_num_str, content = line.split('|', 1)\n",
    "            page_num = int(page_num_str.strip())\n",
    "            content = content.strip()\n",
    "            \n",
    "            if len(content) < 30:\n",
    "                continue\n",
    "            \n",
    "            # Obtener informaci√≥n de la secci√≥n\n",
    "            section_number, section_title = get_section_for_page(page_num)\n",
    "            \n",
    "            # Crear chunks de esta p√°gina\n",
    "            page_chunks = chunk_text_with_overlap(content, chunk_size, overlap)\n",
    "            \n",
    "            for chunk_idx, chunk_text in enumerate(page_chunks):\n",
    "                chunk_data = {\n",
    "                    \"chunk_id\": f\"page{page_num}_chunk{chunk_idx + 1}\",\n",
    "                    \"page_number\": page_num,\n",
    "                    \"chunk_index\": chunk_idx + 1,\n",
    "                    \"content\": chunk_text,\n",
    "                    \"char_count\": len(chunk_text),\n",
    "                    \"word_count\": len(chunk_text.split()),\n",
    "                    \"source\": \"guia_embarazo_parto_clean.pdf\",\n",
    "                    \"section_number\": section_number,\n",
    "                    \"section_title\": section_title\n",
    "                }\n",
    "                chunks_data.append(chunk_data)\n",
    "        \n",
    "        # Guardar chunks\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        file_size = output_path.stat().st_size / (1024*1024)\n",
    "        print(f\"‚úÖ Chunks creados: {len(chunks_data)} chunks ({file_size:.2f} MB)\")\n",
    "        return len(chunks_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creando chunks: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def chunk_text_with_overlap(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"Divide texto en chunks con overlap\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        if end >= len(text):\n",
    "            chunk = text[start:]\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk.strip())\n",
    "            break\n",
    "        \n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Intentar terminar en punto o espacio\n",
    "        if end < len(text) and text[end] not in [' ', '.', ',', ';']:\n",
    "            last_sentence = max(chunk.rfind('.'), chunk.rfind('!'), chunk.rfind('?'))\n",
    "            if last_sentence > chunk_size * 0.6:\n",
    "                chunk = chunk[:last_sentence + 1]\n",
    "                end = start + last_sentence + 1\n",
    "            else:\n",
    "                last_space = chunk.rfind(' ')\n",
    "                if last_space > chunk_size * 0.7:\n",
    "                    chunk = chunk[:last_space]\n",
    "                    end = start + last_space\n",
    "        \n",
    "        chunks.append(chunk.strip())\n",
    "        start = max(end - overlap, start + 1)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3d0a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO PREPROCESAMIENTO COMPLETO ===\n",
      "\n",
      "üîÑ PASO 1: Limpiando PDF...\n",
      "üìÑ P√°ginas originales: 623\n",
      "üóëÔ∏è Eliminando p√°ginas 1-123 (primeras)\n",
      "üóëÔ∏è Eliminando p√°ginas 526-623 (√∫ltimas)\n",
      "üóëÔ∏è Eliminando p√°gina 401 (RECOMENDACIONES) = √≠ndice ajustado 277\n",
      "üóëÔ∏è Saltando p√°gina 401 (RECOMENDACIONES)\n",
      "‚úÖ PDF limpio creado: 401 p√°ginas (1.95 MB)\n",
      "‚úÖ PDF limpio creado correctamente\n",
      "\n",
      "üîÑ PASO 2: Extrayendo texto limpio...\n",
      "üìÑ Extrayendo texto de 401 p√°ginas...\n",
      "üóëÔ∏è Saltando p√°gina 401 (RECOMENDACIONES)\n",
      "‚úÖ PDF limpio creado: 401 p√°ginas (1.95 MB)\n",
      "‚úÖ PDF limpio creado correctamente\n",
      "\n",
      "üîÑ PASO 2: Extrayendo texto limpio...\n",
      "üìÑ Extrayendo texto de 401 p√°ginas...\n",
      "   ‚úì 50/401 p√°ginas procesadas\n",
      "   ‚úì 100/401 p√°ginas procesadas\n",
      "   ‚úì 50/401 p√°ginas procesadas\n",
      "   ‚úì 100/401 p√°ginas procesadas\n",
      "   ‚úì 150/401 p√°ginas procesadas\n",
      "   ‚úì 200/401 p√°ginas procesadas\n",
      "   ‚úì 150/401 p√°ginas procesadas\n",
      "   ‚úì 200/401 p√°ginas procesadas\n",
      "   ‚úì 250/401 p√°ginas procesadas\n",
      "   ‚úì 300/401 p√°ginas procesadas\n",
      "   ‚úì 250/401 p√°ginas procesadas\n",
      "   ‚úì 300/401 p√°ginas procesadas\n",
      "   ‚úì 350/401 p√°ginas procesadas\n",
      "   ‚úì 400/401 p√°ginas procesadas\n",
      "‚úÖ Texto limpio creado: 401 p√°ginas (0.78 MB)\n",
      "‚úÖ Texto limpio creado correctamente\n",
      "\n",
      "üîÑ PASO 3: Creando chunks...\n",
      "   ‚úì 350/401 p√°ginas procesadas\n",
      "   ‚úì 400/401 p√°ginas procesadas\n",
      "‚úÖ Texto limpio creado: 401 p√°ginas (0.78 MB)\n",
      "‚úÖ Texto limpio creado correctamente\n",
      "\n",
      "üîÑ PASO 3: Creando chunks...\n",
      "‚úÖ Chunks creados: 2364 chunks (1.70 MB)\n",
      "‚úÖ Chunks creados correctamente\n",
      "\n",
      "üéâ ¬°PREPROCESAMIENTO COMPLETADO!\n",
      "\n",
      "üìã RESUMEN:\n",
      "   üìÑ PDF limpio: processed\\guia_embarazo_parto_clean.pdf\n",
      "   üìù Texto limpio: processed\\guia_embarazo_clean.txt\n",
      "   üß© Chunks: chunks\\chunks_final.json\n",
      "   üìä P√°ginas procesadas: 401\n",
      "   üìä Chunks generados: 2364\n",
      "\n",
      "üéØ Archivos listos para usar en RAG!\n",
      "‚úÖ Chunks creados: 2364 chunks (1.70 MB)\n",
      "‚úÖ Chunks creados correctamente\n",
      "\n",
      "üéâ ¬°PREPROCESAMIENTO COMPLETADO!\n",
      "\n",
      "üìã RESUMEN:\n",
      "   üìÑ PDF limpio: processed\\guia_embarazo_parto_clean.pdf\n",
      "   üìù Texto limpio: processed\\guia_embarazo_clean.txt\n",
      "   üß© Chunks: chunks\\chunks_final.json\n",
      "   üìä P√°ginas procesadas: 401\n",
      "   üìä Chunks generados: 2364\n",
      "\n",
      "üéØ Archivos listos para usar en RAG!\n"
     ]
    }
   ],
   "source": [
    "# PROCESO PRINCIPAL - Ejecuta todo de una vez\n",
    "print(\"=== INICIANDO PREPROCESAMIENTO COMPLETO ===\")\n",
    "print()\n",
    "\n",
    "if not raw_pdf_path.exists():\n",
    "    print(\"‚ùå Error: Coloca el archivo PDF en la carpeta 'raw/'\")\n",
    "else:\n",
    "    # Paso 1: Limpiar PDF\n",
    "    print(\"üîÑ PASO 1: Limpiando PDF...\")\n",
    "    success_pdf = clean_pdf_direct(raw_pdf_path, clean_pdf_path)\n",
    "    \n",
    "    if success_pdf:\n",
    "        print(\"‚úÖ PDF limpio creado correctamente\")\n",
    "        print()\n",
    "        \n",
    "        # Paso 2: Extraer texto limpio\n",
    "        print(\"üîÑ PASO 2: Extrayendo texto limpio...\")\n",
    "        pages_extracted = extract_clean_text(clean_pdf_path, clean_text_path)\n",
    "        \n",
    "        if pages_extracted > 0:\n",
    "            print(\"‚úÖ Texto limpio creado correctamente\")\n",
    "            print()\n",
    "            \n",
    "            # Paso 3: Crear chunks\n",
    "            print(\"üîÑ PASO 3: Creando chunks...\")\n",
    "            chunks_created = create_chunks(clean_text_path, chunks_path)\n",
    "            \n",
    "            if chunks_created > 0:\n",
    "                print(\"‚úÖ Chunks creados correctamente\")\n",
    "                print()\n",
    "                print(\"üéâ ¬°PREPROCESAMIENTO COMPLETADO!\")\n",
    "                print()\n",
    "                print(\"üìã RESUMEN:\")\n",
    "                print(f\"   üìÑ PDF limpio: {clean_pdf_path}\")\n",
    "                print(f\"   üìù Texto limpio: {clean_text_path}\")\n",
    "                print(f\"   üß© Chunks: {chunks_path}\")\n",
    "                print(f\"   üìä P√°ginas procesadas: {pages_extracted}\")\n",
    "                print(f\"   üìä Chunks generados: {chunks_created}\")\n",
    "                print()\n",
    "                print(\"üéØ Archivos listos para usar en RAG!\")\n",
    "            else:\n",
    "                print(\"‚ùå Error creando chunks\")\n",
    "        else:\n",
    "            print(\"‚ùå Error extrayendo texto\")\n",
    "    else:\n",
    "        print(\"‚ùå Error limpiando PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0c6e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICACI√ìN DE ARCHIVOS GENERADOS ===\n",
      "\n",
      "üìÑ PDF limpio: 401 p√°ginas (1.95 MB)\n",
      "üìù Texto limpio: 401 p√°ginas (0.78 MB)\n",
      "üß© Chunks: 2364 chunks (1.70 MB)\n",
      "üìä Promedio caracteres por chunk: 422\n",
      "‚úÖ Metadatos de secci√≥n incluidos:\n",
      "   ‚Ä¢ section_number: 1\n",
      "   ‚Ä¢ section_title: PREVENCI√ìN Y DETECCI√ìN TEMPRANA DE LAS ALTERACIONES DEL EMBARAZO\n",
      "\n",
      "‚úÖ Verificaci√≥n completada\n"
     ]
    }
   ],
   "source": [
    "# VERIFICACI√ìN OPCIONAL - Revisar archivos generados\n",
    "print(\"=== VERIFICACI√ìN DE ARCHIVOS GENERADOS ===\")\n",
    "print()\n",
    "\n",
    "# Verificar PDF limpio\n",
    "if clean_pdf_path.exists():\n",
    "    try:\n",
    "        with open(clean_pdf_path, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            pages = len(pdf_reader.pages)\n",
    "            size_mb = clean_pdf_path.stat().st_size / (1024*1024)\n",
    "        print(f\"üìÑ PDF limpio: {pages} p√°ginas ({size_mb:.2f} MB)\")\n",
    "    except:\n",
    "        print(\"‚ùå Error verificando PDF\")\n",
    "else:\n",
    "    print(\"‚ùå PDF limpio no encontrado\")\n",
    "\n",
    "# Verificar texto limpio\n",
    "if clean_text_path.exists():\n",
    "    try:\n",
    "        with open(clean_text_path, 'r', encoding='utf-8') as f:\n",
    "            lines = len(f.readlines())\n",
    "        size_mb = clean_text_path.stat().st_size / (1024*1024)\n",
    "        print(f\"üìù Texto limpio: {lines} p√°ginas ({size_mb:.2f} MB)\")\n",
    "    except:\n",
    "        print(\"‚ùå Error verificando texto\")\n",
    "else:\n",
    "    print(\"‚ùå Texto limpio no encontrado\")\n",
    "\n",
    "# Verificar chunks\n",
    "if chunks_path.exists():\n",
    "    try:\n",
    "        with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "            chunks_data = json.load(f)\n",
    "        size_mb = chunks_path.stat().st_size / (1024*1024)\n",
    "        print(f\"üß© Chunks: {len(chunks_data)} chunks ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        if chunks_data:\n",
    "            avg_chars = sum(c['char_count'] for c in chunks_data) // len(chunks_data)\n",
    "            print(f\"üìä Promedio caracteres por chunk: {avg_chars}\")\n",
    "            \n",
    "            # Verificar solo section_number y section_title\n",
    "            sample_chunk = chunks_data[0]\n",
    "            if 'section_number' in sample_chunk and 'section_title' in sample_chunk:\n",
    "                print(\"‚úÖ Metadatos de secci√≥n incluidos:\")\n",
    "                print(f\"   ‚Ä¢ section_number: {sample_chunk['section_number']}\")\n",
    "                print(f\"   ‚Ä¢ section_title: {sample_chunk['section_title']}\")\n",
    "            else:\n",
    "                print(\"‚ùå Metadatos de secci√≥n NO encontrados\")\n",
    "    except:\n",
    "        print(\"‚ùå Error verificando chunks\")\n",
    "else:\n",
    "    print(\"‚ùå Chunks no encontrados\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Verificaci√≥n completada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
