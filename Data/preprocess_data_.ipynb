{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a03e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPROCESAMIENTO SIMPLIFICADO ===\n",
      "ğŸ“ PDF original: raw\\guia_embarazo_parto_2023.pdf\n",
      "ğŸ“ PDF limpio: processed\\guia_embarazo_parto_clean.pdf\n",
      "ğŸ“ Texto limpio: processed\\guia_embarazo_clean.txt\n",
      "ğŸ“ Chunks finales: chunks\\chunks_final.json\n",
      "\n",
      "âœ… Archivo original encontrado (3.54 MB)\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento simplificado de datos - GuÃ­a de Embarazo y Parto 2023\n",
    "# Genera directamente los archivos finales necesarios\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ConfiguraciÃ³n de rutas\n",
    "raw_pdf_path = Path(\"raw/guia_embarazo_parto_2023.pdf\")\n",
    "clean_pdf_path = Path(\"processed/guia_embarazo_parto_clean.pdf\")\n",
    "clean_text_path = Path(\"processed/guia_embarazo_clean.txt\")\n",
    "chunks_path = Path(\"chunks/chunks_final.json\")\n",
    "\n",
    "# Crear directorios si no existen\n",
    "clean_pdf_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "clean_text_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "chunks_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=== PREPROCESAMIENTO SIMPLIFICADO ===\")\n",
    "print(f\"ğŸ“ PDF original: {raw_pdf_path}\")\n",
    "print(f\"ğŸ“ PDF limpio: {clean_pdf_path}\")\n",
    "print(f\"ğŸ“ Texto limpio: {clean_text_path}\")\n",
    "print(f\"ğŸ“ Chunks finales: {chunks_path}\")\n",
    "print()\n",
    "\n",
    "# Verificar archivo original\n",
    "if raw_pdf_path.exists():\n",
    "    size_mb = raw_pdf_path.stat().st_size / (1024*1024)\n",
    "    print(f\"âœ… Archivo original encontrado ({size_mb:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"âŒ Error: No se encuentra {raw_pdf_path}\")\n",
    "    print(\"   Coloca el archivo PDF original en la carpeta 'raw/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db475bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pdf_direct(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Limpia el PDF directamente: \n",
    "    - Elimina primeras 123 pÃ¡ginas \n",
    "    - Elimina Ãºltimas 98 pÃ¡ginas\n",
    "    - Elimina pÃ¡gina 401 especÃ­ficamente (RECOMENDACIONES PARA LA IMPLEMENTACIÃ“N)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_path, 'rb') as input_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(input_file)\n",
    "            pdf_writer = PyPDF2.PdfWriter()\n",
    "            \n",
    "            total_pages = len(pdf_reader.pages)\n",
    "            print(f\"ğŸ“„ PÃ¡ginas originales: {total_pages}\")\n",
    "            \n",
    "            # Calcular pÃ¡ginas a mantener\n",
    "            start_page = 123  # Eliminar primeras 123 pÃ¡ginas (Ã­ndice 0-122)\n",
    "            end_page = total_pages - 98  # Eliminar Ãºltimas 98 pÃ¡ginas\n",
    "            \n",
    "            print(f\"ğŸ—‘ï¸ Eliminando pÃ¡ginas 1-{start_page} (primeras)\")\n",
    "            print(f\"ğŸ—‘ï¸ Eliminando pÃ¡ginas {end_page + 1}-{total_pages} (Ãºltimas)\")\n",
    "            \n",
    "            # Eliminar pÃ¡gina 401 especÃ­ficamente\n",
    "            # En el PDF original, despuÃ©s de eliminar las primeras 123 pÃ¡ginas,\n",
    "            # la pÃ¡gina 401 del visualizador serÃ¡ el Ã­ndice 401-1=400 en base 0\n",
    "            # Pero ajustado por eliminar las primeras 123, serÃ¡ 400-123=277\n",
    "            exclude_page_adjusted = 401 - 123 - 1  # = 277 (Ã­ndice 0-based despuÃ©s del ajuste)\n",
    "            print(f\"ğŸ—‘ï¸ Eliminando pÃ¡gina 401 (RECOMENDACIONES) = Ã­ndice ajustado {exclude_page_adjusted}\")\n",
    "            \n",
    "            # Agregar pÃ¡ginas vÃ¡lidas al PDF limpio\n",
    "            pages_added = 0\n",
    "            for page_num in range(start_page, end_page):\n",
    "                adjusted_index = page_num - start_page\n",
    "                if adjusted_index != exclude_page_adjusted:\n",
    "                    pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "                    pages_added += 1\n",
    "                else:\n",
    "                    print(f\"ğŸ—‘ï¸ Saltando pÃ¡gina 401 (RECOMENDACIONES)\")\n",
    "            \n",
    "            # Guardar PDF limpio\n",
    "            with open(output_path, 'wb') as output_file:\n",
    "                pdf_writer.write(output_file)\n",
    "            \n",
    "            final_size = output_path.stat().st_size / (1024*1024)\n",
    "            print(f\"âœ… PDF limpio creado: {pages_added} pÃ¡ginas ({final_size:.2f} MB)\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_text(pdf_path, output_path):\n",
    "    \"\"\"\n",
    "    Extrae texto limpio del PDF en formato PÃGINA|CONTENIDO\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        \n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        total_pages = len(doc)\n",
    "        print(f\"ğŸ“„ Extrayendo texto de {total_pages} pÃ¡ginas...\")\n",
    "        \n",
    "        clean_lines = []\n",
    "        \n",
    "        for page_num in range(total_pages):\n",
    "            page = doc.load_page(page_num)\n",
    "            page_text = page.get_text()\n",
    "            \n",
    "            # Limpiar texto de la pÃ¡gina\n",
    "            clean_text = clean_page_content(page_text)\n",
    "            \n",
    "            # Solo agregar pÃ¡ginas con contenido vÃ¡lido\n",
    "            if len(clean_text) > 30:\n",
    "                line = f\"{page_num + 1}|{clean_text}\"\n",
    "                clean_lines.append(line)\n",
    "            \n",
    "            if (page_num + 1) % 50 == 0:\n",
    "                print(f\"   âœ“ {page_num + 1}/{total_pages} pÃ¡ginas procesadas\")\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Guardar texto limpio\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for line in clean_lines:\n",
    "                f.write(line + '\\n')\n",
    "        \n",
    "        file_size = output_path.stat().st_size / (1024*1024)\n",
    "        print(f\"âœ… Texto limpio creado: {len(clean_lines)} pÃ¡ginas ({file_size:.2f} MB)\")\n",
    "        return len(clean_lines)\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âŒ Error: Instala PyMuPDF con: pip install PyMuPDF\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def clean_page_content(text):\n",
    "    \"\"\"Limpia el contenido de una pÃ¡gina\"\"\"\n",
    "    if not text or len(text.strip()) < 5:\n",
    "        return \"\"\n",
    "    \n",
    "    # Eliminar letras de recomendaciÃ³n aisladas (A, B, C, D, R, V) al inicio de lÃ­neas\n",
    "    # Esto es especÃ­fico para las tablas de recomendaciones\n",
    "    text = re.sub(r'^\\s*[A-DRV]\\s+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Normalizar espacios, pero manteniendo saltos de lÃ­nea para el procesamiento anterior\n",
    "    text = text.replace('\\u00A0', ' ').replace('\\u2028', ' ').replace('\\u2029', ' ')\n",
    "    \n",
    "    # Eliminar marcadores de evidencia y sÃ­mbolos\n",
    "    # Formatos: 1++, 1+, 1-, 2++, 2+, 2-, 3, 4\n",
    "    text = re.sub(r'\\b\\d[+\\-]{1,2}\\b', '', text)\n",
    "    # Formatos: Ia, Ib, II, III, IV\n",
    "    text = re.sub(r'\\b(Ia|Ib|II|III|IV)\\b', '', text)\n",
    "    # Formatos numÃ©ricos y con cruces que quedaron\n",
    "    text = re.sub(r'\\+\\d+\\s*', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\+\\+?\\b', '', text)\n",
    "    text = re.sub(r'\\b[IVX]+\\+?\\b', '', text)\n",
    "    text = re.sub(r'[-_â•â”€â–¬â– |â–Œâ–â”ƒ]{3,}', '', text)\n",
    "    text = text.replace('âˆš', '').replace('â†‘', '')\n",
    "    \n",
    "    # Eliminar nÃºmeros de pÃ¡gina y referencias entre parÃ©ntesis\n",
    "    text = re.sub(r'^\\d{1,3}\\s+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\s+\\d{1,3}$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\(\\d{1,4}\\)', '', text)\n",
    "    \n",
    "    # Finalmente, colapsar todos los espacios mÃºltiples a uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be5c4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text_file_path, output_path, chunk_size=500, overlap=100):\n",
    "    \"\"\"\n",
    "    Crea chunks directamente del archivo de texto limpio con section_number y section_title\n",
    "    \"\"\"\n",
    "    # Definir secciones del documento\n",
    "    SECTIONS = {\n",
    "        1: {\n",
    "            \"title\": \"PREVENCIÃ“N Y DETECCIÃ“N TEMPRANA DE LAS ALTERACIONES DEL EMBARAZO\",\n",
    "            \"start_page\": 1,\n",
    "            \"end_page\": 145\n",
    "        },\n",
    "        2: {\n",
    "            \"title\": \"ABORDAJE DE LAS COMPLICACIONES HIPERTENSIVAS ASOCIADAS AL EMBARAZO\", \n",
    "            \"start_page\": 146,\n",
    "            \"end_page\": 201\n",
    "        },\n",
    "        3: {\n",
    "            \"title\": \"INFECCIONES EN EL EMBARAZO: RUPTURA PREMATURA DE MEMBRANAS (RPM)\",\n",
    "            \"start_page\": 202,\n",
    "            \"end_page\": 277\n",
    "        },\n",
    "        4: {\n",
    "            \"title\": \"NUTRICIÃ“N EN EL EMBARAZO\",\n",
    "            \"start_page\": 278,\n",
    "            \"end_page\": 310\n",
    "        },\n",
    "        5: {\n",
    "            \"title\": \"DETECCIÃ“N TEMPRANA DE LAS ANOMALÃAS DURANTE EL TRABAJO DE PARTO, ATENCIÃ“N DEL PARTO NORMAL Y DISTÃ“CICO\",\n",
    "            \"start_page\": 311,\n",
    "            \"end_page\": 357\n",
    "        },\n",
    "        6: {\n",
    "            \"title\": \"COMPLICACIONES HEMORRÃGICAS ASOCIADAS AL EMBARAZO\",\n",
    "            \"start_page\": 358,\n",
    "            \"end_page\": 400\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def get_section_for_page(page_num):\n",
    "        \"\"\"Determina a quÃ© secciÃ³n pertenece una pÃ¡gina\"\"\"\n",
    "        for section_num, section_info in SECTIONS.items():\n",
    "            if section_info[\"start_page\"] <= page_num <= section_info[\"end_page\"]:\n",
    "                return section_num, section_info[\"title\"]\n",
    "        return 1, SECTIONS[1][\"title\"]  # Por defecto secciÃ³n 1\n",
    "    \n",
    "    try:\n",
    "        with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        chunks_data = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line or '|' not in line:\n",
    "                continue\n",
    "            \n",
    "            page_num_str, content = line.split('|', 1)\n",
    "            page_num = int(page_num_str.strip())\n",
    "            content = content.strip()\n",
    "            \n",
    "            if len(content) < 30:\n",
    "                continue\n",
    "            \n",
    "            # Obtener informaciÃ³n de la secciÃ³n\n",
    "            section_number, section_title = get_section_for_page(page_num)\n",
    "            \n",
    "            # Crear chunks de esta pÃ¡gina\n",
    "            page_chunks = chunk_text_with_overlap(content, chunk_size, overlap)\n",
    "            \n",
    "            for chunk_idx, chunk_text in enumerate(page_chunks):\n",
    "                chunk_data = {\n",
    "                    \"chunk_id\": f\"page{page_num}_chunk{chunk_idx + 1}\",\n",
    "                    \"page_number\": page_num,\n",
    "                    \"chunk_index\": chunk_idx + 1,\n",
    "                    \"content\": chunk_text,\n",
    "                    \"char_count\": len(chunk_text),\n",
    "                    \"word_count\": len(chunk_text.split()),\n",
    "                    \"source\": \"guia_embarazo_parto_clean.pdf\",\n",
    "                    \"section_number\": section_number,\n",
    "                    \"section_title\": section_title\n",
    "                }\n",
    "                chunks_data.append(chunk_data)\n",
    "        \n",
    "        # Guardar chunks\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        file_size = output_path.stat().st_size / (1024*1024)\n",
    "        print(f\"âœ… Chunks creados: {len(chunks_data)} chunks ({file_size:.2f} MB)\")\n",
    "        return len(chunks_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creando chunks: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def chunk_text_with_overlap(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"Divide texto en chunks con overlap\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        if end >= len(text):\n",
    "            chunk = text[start:]\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk.strip())\n",
    "            break\n",
    "        \n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Intentar terminar en punto o espacio\n",
    "        if end < len(text) and text[end] not in [' ', '.', ',', ';']:\n",
    "            last_sentence = max(chunk.rfind('.'), chunk.rfind('!'), chunk.rfind('?'))\n",
    "            if last_sentence > chunk_size * 0.6:\n",
    "                chunk = chunk[:last_sentence + 1]\n",
    "                end = start + last_sentence + 1\n",
    "            else:\n",
    "                last_space = chunk.rfind(' ')\n",
    "                if last_space > chunk_size * 0.7:\n",
    "                    chunk = chunk[:last_space]\n",
    "                    end = start + last_space\n",
    "        \n",
    "        chunks.append(chunk.strip())\n",
    "        start = max(end - overlap, start + 1)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f3d0a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO PREPROCESAMIENTO COMPLETO ===\n",
      "\n",
      "ğŸ”„ PASO 1: Limpiando PDF...\n",
      "ğŸ“„ PÃ¡ginas originales: 623\n",
      "ğŸ—‘ï¸ Eliminando pÃ¡ginas 1-123 (primeras)\n",
      "ğŸ—‘ï¸ Eliminando pÃ¡ginas 526-623 (Ãºltimas)\n",
      "ğŸ—‘ï¸ Eliminando pÃ¡gina 401 (RECOMENDACIONES) = Ã­ndice ajustado 277\n",
      "ğŸ—‘ï¸ Saltando pÃ¡gina 401 (RECOMENDACIONES)\n",
      "âœ… PDF limpio creado: 401 pÃ¡ginas (1.95 MB)\n",
      "âœ… PDF limpio creado correctamente\n",
      "\n",
      "ğŸ”„ PASO 2: Extrayendo texto limpio...\n",
      "ğŸ“„ Extrayendo texto de 401 pÃ¡ginas...\n",
      "ğŸ—‘ï¸ Saltando pÃ¡gina 401 (RECOMENDACIONES)\n",
      "âœ… PDF limpio creado: 401 pÃ¡ginas (1.95 MB)\n",
      "âœ… PDF limpio creado correctamente\n",
      "\n",
      "ğŸ”„ PASO 2: Extrayendo texto limpio...\n",
      "ğŸ“„ Extrayendo texto de 401 pÃ¡ginas...\n",
      "   âœ“ 50/401 pÃ¡ginas procesadas\n",
      "   âœ“ 100/401 pÃ¡ginas procesadas\n",
      "   âœ“ 50/401 pÃ¡ginas procesadas\n",
      "   âœ“ 100/401 pÃ¡ginas procesadas\n",
      "   âœ“ 150/401 pÃ¡ginas procesadas\n",
      "   âœ“ 200/401 pÃ¡ginas procesadas\n",
      "   âœ“ 150/401 pÃ¡ginas procesadas\n",
      "   âœ“ 200/401 pÃ¡ginas procesadas\n",
      "   âœ“ 250/401 pÃ¡ginas procesadas\n",
      "   âœ“ 300/401 pÃ¡ginas procesadas\n",
      "   âœ“ 250/401 pÃ¡ginas procesadas\n",
      "   âœ“ 300/401 pÃ¡ginas procesadas\n",
      "   âœ“ 350/401 pÃ¡ginas procesadas\n",
      "   âœ“ 400/401 pÃ¡ginas procesadas\n",
      "âœ… Texto limpio creado: 401 pÃ¡ginas (0.78 MB)\n",
      "âœ… Texto limpio creado correctamente\n",
      "\n",
      "ğŸ”„ PASO 3: Creando chunks...\n",
      "   âœ“ 350/401 pÃ¡ginas procesadas\n",
      "   âœ“ 400/401 pÃ¡ginas procesadas\n",
      "âœ… Texto limpio creado: 401 pÃ¡ginas (0.78 MB)\n",
      "âœ… Texto limpio creado correctamente\n",
      "\n",
      "ğŸ”„ PASO 3: Creando chunks...\n",
      "âœ… Chunks creados: 2364 chunks (1.70 MB)\n",
      "âœ… Chunks creados correctamente\n",
      "\n",
      "ğŸ‰ Â¡PREPROCESAMIENTO COMPLETADO!\n",
      "\n",
      "ğŸ“‹ RESUMEN:\n",
      "   ğŸ“„ PDF limpio: processed\\guia_embarazo_parto_clean.pdf\n",
      "   ğŸ“ Texto limpio: processed\\guia_embarazo_clean.txt\n",
      "   ğŸ§© Chunks: chunks\\chunks_final.json\n",
      "   ğŸ“Š PÃ¡ginas procesadas: 401\n",
      "   ğŸ“Š Chunks generados: 2364\n",
      "\n",
      "ğŸ¯ Archivos listos para usar en RAG!\n",
      "âœ… Chunks creados: 2364 chunks (1.70 MB)\n",
      "âœ… Chunks creados correctamente\n",
      "\n",
      "ğŸ‰ Â¡PREPROCESAMIENTO COMPLETADO!\n",
      "\n",
      "ğŸ“‹ RESUMEN:\n",
      "   ğŸ“„ PDF limpio: processed\\guia_embarazo_parto_clean.pdf\n",
      "   ğŸ“ Texto limpio: processed\\guia_embarazo_clean.txt\n",
      "   ğŸ§© Chunks: chunks\\chunks_final.json\n",
      "   ğŸ“Š PÃ¡ginas procesadas: 401\n",
      "   ğŸ“Š Chunks generados: 2364\n",
      "\n",
      "ğŸ¯ Archivos listos para usar en RAG!\n"
     ]
    }
   ],
   "source": [
    "# PROCESO PRINCIPAL - Ejecuta todo de una vez\n",
    "print(\"=== INICIANDO PREPROCESAMIENTO COMPLETO ===\")\n",
    "print()\n",
    "\n",
    "if not raw_pdf_path.exists():\n",
    "    print(\"âŒ Error: Coloca el archivo PDF en la carpeta 'raw/'\")\n",
    "else:\n",
    "    # Paso 1: Limpiar PDF\n",
    "    print(\"ğŸ”„ PASO 1: Limpiando PDF...\")\n",
    "    success_pdf = clean_pdf_direct(raw_pdf_path, clean_pdf_path)\n",
    "    \n",
    "    if success_pdf:\n",
    "        print(\"âœ… PDF limpio creado correctamente\")\n",
    "        print()\n",
    "        \n",
    "        # Paso 2: Extraer texto limpio\n",
    "        print(\"ğŸ”„ PASO 2: Extrayendo texto limpio...\")\n",
    "        pages_extracted = extract_clean_text(clean_pdf_path, clean_text_path)\n",
    "        \n",
    "        if pages_extracted > 0:\n",
    "            print(\"âœ… Texto limpio creado correctamente\")\n",
    "            print()\n",
    "            \n",
    "            # Paso 3: Crear chunks\n",
    "            print(\"ğŸ”„ PASO 3: Creando chunks...\")\n",
    "            chunks_created = create_chunks(clean_text_path, chunks_path)\n",
    "            \n",
    "            if chunks_created > 0:\n",
    "                print(\"âœ… Chunks creados correctamente\")\n",
    "                print()\n",
    "                print(\"ğŸ‰ Â¡PREPROCESAMIENTO COMPLETADO!\")\n",
    "                print()\n",
    "                print(\"ğŸ“‹ RESUMEN:\")\n",
    "                print(f\"   ğŸ“„ PDF limpio: {clean_pdf_path}\")\n",
    "                print(f\"   ğŸ“ Texto limpio: {clean_text_path}\")\n",
    "                print(f\"   ğŸ§© Chunks: {chunks_path}\")\n",
    "                print(f\"   ğŸ“Š PÃ¡ginas procesadas: {pages_extracted}\")\n",
    "                print(f\"   ğŸ“Š Chunks generados: {chunks_created}\")\n",
    "                print()\n",
    "                print(\"ğŸ¯ Archivos listos para usar en RAG!\")\n",
    "            else:\n",
    "                print(\"âŒ Error creando chunks\")\n",
    "        else:\n",
    "            print(\"âŒ Error extrayendo texto\")\n",
    "    else:\n",
    "        print(\"âŒ Error limpiando PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0c6e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICACIÃ“N DE ARCHIVOS GENERADOS ===\n",
      "\n",
      "ğŸ“„ PDF limpio: 401 pÃ¡ginas (1.95 MB)\n",
      "ğŸ“ Texto limpio: 401 pÃ¡ginas (0.78 MB)\n",
      "ğŸ§© Chunks: 2364 chunks (1.70 MB)\n",
      "ğŸ“Š Promedio caracteres por chunk: 422\n",
      "âœ… Metadatos de secciÃ³n incluidos:\n",
      "   â€¢ section_number: 1\n",
      "   â€¢ section_title: PREVENCIÃ“N Y DETECCIÃ“N TEMPRANA DE LAS ALTERACIONES DEL EMBARAZO\n",
      "\n",
      "âœ… VerificaciÃ³n completada\n"
     ]
    }
   ],
   "source": [
    "# VERIFICACIÃ“N OPCIONAL - Revisar archivos generados\n",
    "print(\"=== VERIFICACIÃ“N DE ARCHIVOS GENERADOS ===\")\n",
    "print()\n",
    "\n",
    "# Verificar PDF limpio\n",
    "if clean_pdf_path.exists():\n",
    "    try:\n",
    "        with open(clean_pdf_path, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            pages = len(pdf_reader.pages)\n",
    "            size_mb = clean_pdf_path.stat().st_size / (1024*1024)\n",
    "        print(f\"ğŸ“„ PDF limpio: {pages} pÃ¡ginas ({size_mb:.2f} MB)\")\n",
    "    except:\n",
    "        print(\"âŒ Error verificando PDF\")\n",
    "else:\n",
    "    print(\"âŒ PDF limpio no encontrado\")\n",
    "\n",
    "# Verificar texto limpio\n",
    "if clean_text_path.exists():\n",
    "    try:\n",
    "        with open(clean_text_path, 'r', encoding='utf-8') as f:\n",
    "            lines = len(f.readlines())\n",
    "        size_mb = clean_text_path.stat().st_size / (1024*1024)\n",
    "        print(f\"ğŸ“ Texto limpio: {lines} pÃ¡ginas ({size_mb:.2f} MB)\")\n",
    "    except:\n",
    "        print(\"âŒ Error verificando texto\")\n",
    "else:\n",
    "    print(\"âŒ Texto limpio no encontrado\")\n",
    "\n",
    "# Verificar chunks\n",
    "if chunks_path.exists():\n",
    "    try:\n",
    "        with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "            chunks_data = json.load(f)\n",
    "        size_mb = chunks_path.stat().st_size / (1024*1024)\n",
    "        print(f\"ğŸ§© Chunks: {len(chunks_data)} chunks ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        if chunks_data:\n",
    "            avg_chars = sum(c['char_count'] for c in chunks_data) // len(chunks_data)\n",
    "            print(f\"ğŸ“Š Promedio caracteres por chunk: {avg_chars}\")\n",
    "            \n",
    "            # Verificar solo section_number y section_title\n",
    "            sample_chunk = chunks_data[0]\n",
    "            if 'section_number' in sample_chunk and 'section_title' in sample_chunk:\n",
    "                print(\"âœ… Metadatos de secciÃ³n incluidos:\")\n",
    "                print(f\"   â€¢ section_number: {sample_chunk['section_number']}\")\n",
    "                print(f\"   â€¢ section_title: {sample_chunk['section_title']}\")\n",
    "            else:\n",
    "                print(\"âŒ Metadatos de secciÃ³n NO encontrados\")\n",
    "    except:\n",
    "        print(\"âŒ Error verificando chunks\")\n",
    "else:\n",
    "    print(\"âŒ Chunks no encontrados\")\n",
    "\n",
    "print()\n",
    "print(\"âœ… VerificaciÃ³n completada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
